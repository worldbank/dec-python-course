{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0d8aaea-2a0d-4929-ab13-fd00a496b023",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction to Data Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3fd18aa-d600-413b-9503-d678cdd69c24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this session, we will explore the fundamentals of building **data pipelines** in Python using **PySpark** with a focus on **reproducibility**, **reliability**, and **efficient** workflow orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b15dc7-0989-4b8e-a92a-e4ce3baaae0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What is a data pipeline?\n",
    "\n",
    "A data pipeline is a series of processes that automate the movement and transformation of data from one or more sources to a final destination, where it can be used for analysis. A data pipeline ensures the reliable, reproducible, and efficient flow of data through different stages, while avoiding redundant processing and supporting scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903214eb-db62-4b87-b069-f587bd7e4acf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Why data pipelines?\n",
    "\n",
    "Raw data often comes from multiple sources, which can be messy and inconsistent. Data pipelines are crucial in this context because they automate and standardize the process of Extracting, Transforming, and Loading data, making it ready for analysis or or other downstream purposes. \n",
    "\n",
    "Consider the cases when you obtain data from an Organization/ Government for a specific year. You work through cleaning the data and produce some analysis. The next year new data comes in, potentially similar but with minor modiifcations, you should be able to reuse the work from the previous year and make consistent decisions on how the data is to be treated. These are the scenarios where data pipelines are most useful.  They are crucial for automating the extraction, cleaning, transformation, and export of data, enabling more **efficient**, **reliable**, and **repeatable** data workflows. \n",
    "\n",
    "In addition to this, we maintain a clear **lineage** of data -- which makes transparent the changes made to the raw data and what transformations have been applied to obtain the final dataset. In databricks it is possible to configure controls such that the lineage of each column in a table can be traced schematically. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do we already use data pipelines? Or are they completely new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code snippet below, we obtain the some subnational population data by making a requests call to a specific url from census.gov. Then, we manipulate this data using pandas to get it in the form we require and retaining the relevant columns. Finally, we *save* this data. This code is taken from [here](subnational_population.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"data_pipelines_tutorial\"\n",
    "URL = 'https://www2.census.gov/programs-surveys/international-programs/tables/time-series/prh/kenya.xlsx'\n",
    "\n",
    "df_raw = pd.read_excel(URL, sheet_name='2000 - 2040', skiprows=2, header=None)\n",
    "# Read correct header\n",
    "header = df_raw.iloc[1]\n",
    "df_raw.columns = header\n",
    "df_raw = df_raw.drop([0,1,2])\n",
    "\n",
    "# Extract Total population columns \n",
    "df_pop_wide = df_raw[df_raw.ADM_LEVEL==1][['CNTRY_NAME', 'ADM1_NAME']+[x for x in header if 'BTOTL' in x]]\n",
    "df_pop = pd.melt(df_pop_wide, id_vars=['CNTRY_NAME', 'ADM1_NAME'], var_name='year', value_name='population')\n",
    "df_pop['year'] = df_pop['year'].str.extract(r'(\\d+)').astype(int)\n",
    "df_pop.columns = ['country_name', 'adm1_name', 'year', 'population']\n",
    "\n",
    "# Modifications to the admin1 and county name and add data_source\n",
    "name_standardization = {\n",
    "    ''\n",
    "}\n",
    "df_pop['country_name'] = df_pop['country_name'].str.title()\n",
    "df_pop['adm1_name'] = df_pop['adm1_name'].str.replace(r'[-/]+', ' ', regex=True).str.title()\n",
    "df_pop = df_pop.astype({'year': 'int', 'population': 'int'})\n",
    "df_pop = df_pop.sort_values(['adm1_name', 'year'], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "# Save to subnational population table\n",
    "spark.createDataFrame(df_pop).write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.kenya_subnational_population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9401cee-6726-4a5c-b267-039fea03e8d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## What are the building blocks of a Data Pipeline?\n",
    "\n",
    "A data pipeline is a structured framework that enables the flow of data from source to destination, encompassing several key processes. Specific implementation may vary but the fundamental components of a data pipeline can be abstracted as follows:\n",
    "\n",
    "**Data Ingestion:**\n",
    "\n",
    "Collect data from various sources, such as databases, APIs, or files.This initial step involves loading raw data into the system. For example, reading data from an Excel file, a CSV file, or pulling data from an API. For illustration with an example we will ingest data from Excel files.\n",
    "\n",
    "**ETL (Extract, Transform, Load):**\n",
    "\n",
    "_Extract:_\n",
    "Extract raw data that needs to be processed, often involving multiple formats and structures.\n",
    "\n",
    "_Transform:_\n",
    "This involves cleaning, filtering, enriching, and preparing the data for analysis. Typically this could include creating new columns or aggregating data as well.\n",
    "Transformation ensures that the data is useful, and reliable.\n",
    "\n",
    "_Load:_\n",
    "After transformation, the cleaned data is saved into a destination storage system (like a database or a data lake) for visualization, or further processing.\n",
    "\n",
    "In cases when compute resources are robust, the general order followed is **ELT**. Here the data is loaded in a slightly raw form and the transformations happen after loading the data into the compute infrastructure. This allows one to retain access the large compute resources and also store intermediate forms of the data. In this tutorial we will explore this format with a **medallion** style transformation where the data is treated in different _layers_ each corresponding to a cleaner/more transformed version, generally called *bronze*, *silver* and *gold*.\n",
    "\n",
    "We will explore this in greater detail and practice constructing a pipline in the medallion architecture [below](#Medallion-Architecture)\n",
    "\n",
    "**Orchestration:**\n",
    "\n",
    "This stage manages the execution of the pipeline tasks and dependencies. Orchestration ensures that each task runs in the correct order and handles dependencies, enabling automation and scheduling of the entire pipeline process.\n",
    "\n",
    "Think of each unit of code doing a specific/ collection of specific tasks as a **Task**. Then, Tasks can be linked to define the order in which evaluation of these tasks take place and the dependencies involved. This conceptual linking of tasks into an ordered graph (DAG) allows for complex evaluations to be handled in an orderly manner -- as well as on a schedule. The example below shows one such orchestration view for a specific project. Green implies that the task ran without errors and red implies an error. This allows for easy debuggin and maintainance. \n",
    "\n",
    " In this tutorial we will use the workflows feature in **databricks** for orchestration, although it's possible to consrtuct it in pySpark itself. The implementation for our sample data pipeline process is shown [below](#Orchestration).\n",
    "\n",
    "**Monitoring and Logging:**\n",
    "\n",
    "Implementing monitoring mechanisms to log the pipeline execution, detect failures, and ensure data quality throughout the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae59b692-f796-434d-90cb-579bd4ad0645",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![](assets/pipeline-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0174d479-d9ef-4ec1-92ab-9b74b2acda2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "The goal of the session is to help you understand how to create basic data pipelines to handle typical data processing tasks and how to modularize these steps to improve maintainability and scalability.\n",
    "\n",
    "We will follow a structured approach, starting with data ingestion, then cleaning and transforming the data, and then exporting the processed data and aggregating it to a useful table. Finally, we will use the processed data to produce a plot, to illustrate a typical workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8759368c-870f-41f0-9f54-3f5000757140",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sample Data (Kenya public expenditure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "396b6fd8-0201-488a-8740-d3e818522b44",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For this tutorial we will use the Publicly available Kenya BOOST expenditure dataset available [here](https://datacatalog.worldbank.org/search/dataset/0038086). This data contains spending information at the central and local levels and continues to be yearly updated, curated and disseminated by the Government of Kenya.\n",
    "\n",
    "\n",
    "We will construct a data pipeline that will ingest this data, make certain modifications, enrich it by adding relevant columns and finally save the resulting tables which would be used to produce some plots to further summarize and understand this expenditure data\n",
    "\n",
    "We will use some domain specific understanding of this data, but the process and logic behind the modifications are not domain dependent and illustrate the general principles of data pipeline construction. Transformation operations performed here are typical of the kind of operations encountered when working with raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "770856ce-3c45-4bb5-bdaf-f438583e7aed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Class</th>\n",
       "      <th>Vote Groups</th>\n",
       "      <th>National Government Votes &amp; Counties</th>\n",
       "      <th>Sub Votes</th>\n",
       "      <th>Head/Dept</th>\n",
       "      <th>SubHead/SubDept</th>\n",
       "      <th>County Government Ministries</th>\n",
       "      <th>National/County (Geo1)</th>\n",
       "      <th>Counties (Geo2)</th>\n",
       "      <th>...</th>\n",
       "      <th>SOF2</th>\n",
       "      <th>SOF3</th>\n",
       "      <th>SOF4</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Programme</th>\n",
       "      <th>Sub-programme</th>\n",
       "      <th>Initial Budget (Printed Estimate)</th>\n",
       "      <th>Final Budget (Approved Estimate)</th>\n",
       "      <th>Final Expenditure (Total Payment Comm.)</th>\n",
       "      <th>Unnamed: 35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39293</th>\n",
       "      <td>2019-20</td>\n",
       "      <td>0 Recurrent Expenditure</td>\n",
       "      <td>1 National Government Ministries, Departments ...</td>\n",
       "      <td>118 Ministry of East African Affairs, Commerce...</td>\n",
       "      <td>1185 State Department For Social Protection</td>\n",
       "      <td>11850012 Cash Transfers</td>\n",
       "      <td>1185001202 Cash Transfers - Field Services</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02 Counties</td>\n",
       "      <td>4610 Narok County</td>\n",
       "      <td>...</td>\n",
       "      <td>00 Domestic Resources</td>\n",
       "      <td>00001 Consolidated Fund</td>\n",
       "      <td>00001001 Exchequer ( GOK )</td>\n",
       "      <td>09 Social Protection, Culture and Recreation</td>\n",
       "      <td>0909000000 National Social Safety Net</td>\n",
       "      <td>0909010000 Social Assistance to Vulnerable Groups</td>\n",
       "      <td>0</td>\n",
       "      <td>88326.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505287</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>0 Recurrent Expenditure</td>\n",
       "      <td>1 National Government Ministries, Departments ...</td>\n",
       "      <td>110 Ministry of Environm,ent, Water and Natura...</td>\n",
       "      <td>1109 Ministry of Water &amp; Sanitation and Irriga...</td>\n",
       "      <td>11090022 Land Reclamation Services</td>\n",
       "      <td>1109002201 Land Reclamation Services - HQ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01 National</td>\n",
       "      <td>0000 Nation-Wide</td>\n",
       "      <td>...</td>\n",
       "      <td>00 Domestic Resources</td>\n",
       "      <td>00001 Consolidated Fund</td>\n",
       "      <td>00001001 Exchequer ( GOK )</td>\n",
       "      <td>10 Environment Protection, Water And Natural R...</td>\n",
       "      <td>1014000000 Irrigation and Land Reclamation</td>\n",
       "      <td>1014020000 Land Reclamation</td>\n",
       "      <td>22156</td>\n",
       "      <td>11078.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395201</th>\n",
       "      <td>2021-22</td>\n",
       "      <td>0 Recurrent Expenditure</td>\n",
       "      <td>1 National Government Ministries, Departments ...</td>\n",
       "      <td>107 The National Treasury</td>\n",
       "      <td>1071 The National Treasury</td>\n",
       "      <td>10710019 District Treasuries Services</td>\n",
       "      <td>1071001901 Headquarters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02 Counties</td>\n",
       "      <td>4660 Kajiado County</td>\n",
       "      <td>...</td>\n",
       "      <td>00 Domestic Resources</td>\n",
       "      <td>00001 Consolidated Fund</td>\n",
       "      <td>00001001 Exchequer ( GOK )</td>\n",
       "      <td>07 Public Administration And International Rel...</td>\n",
       "      <td>0718000000 Public Financial Management</td>\n",
       "      <td>0718040000 Accounting Services</td>\n",
       "      <td>0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192100</th>\n",
       "      <td>2020-21</td>\n",
       "      <td>0 Recurrent Expenditure</td>\n",
       "      <td>1 National Government Ministries, Departments ...</td>\n",
       "      <td>107 The National Treasury</td>\n",
       "      <td>1071 The National Treasury</td>\n",
       "      <td>10710019 District Treasuries Services</td>\n",
       "      <td>1071001901 Headquarters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02 Counties</td>\n",
       "      <td>4710 Kericho County</td>\n",
       "      <td>...</td>\n",
       "      <td>00 Domestic Resources</td>\n",
       "      <td>00001 Consolidated Fund</td>\n",
       "      <td>00001001 Exchequer ( GOK )</td>\n",
       "      <td>07 Public Administration And International Rel...</td>\n",
       "      <td>0718000000 Public Financial Management</td>\n",
       "      <td>0718040000 Accounting Services</td>\n",
       "      <td>0</td>\n",
       "      <td>11681.0</td>\n",
       "      <td>11681.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248888</th>\n",
       "      <td>2020-21</td>\n",
       "      <td>0 Recurrent Expenditure</td>\n",
       "      <td>1 National Government Ministries, Departments ...</td>\n",
       "      <td>108 Ministry of Health</td>\n",
       "      <td>1081 Ministry of Health</td>\n",
       "      <td>10810090 Kenya Expanded Programme Immunization</td>\n",
       "      <td>1081009001 Headquarters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01 National</td>\n",
       "      <td>0000 Nation-Wide</td>\n",
       "      <td>...</td>\n",
       "      <td>00 Domestic Resources</td>\n",
       "      <td>00001 Consolidated Fund</td>\n",
       "      <td>00001001 Exchequer ( GOK )</td>\n",
       "      <td>04 Health</td>\n",
       "      <td>0401000000 Preventive, Promotive &amp; Reproductiv...</td>\n",
       "      <td>0401030000 Reproductive Maternal Neo-natal Chi...</td>\n",
       "      <td>900000</td>\n",
       "      <td>900000.0</td>\n",
       "      <td>460000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Year                    Class  \\\n",
       "39293   2019-20  0 Recurrent Expenditure   \n",
       "505287  2021-22  0 Recurrent Expenditure   \n",
       "395201  2021-22  0 Recurrent Expenditure   \n",
       "192100  2020-21  0 Recurrent Expenditure   \n",
       "248888  2020-21  0 Recurrent Expenditure   \n",
       "\n",
       "                                              Vote Groups  \\\n",
       "39293   1 National Government Ministries, Departments ...   \n",
       "505287  1 National Government Ministries, Departments ...   \n",
       "395201  1 National Government Ministries, Departments ...   \n",
       "192100  1 National Government Ministries, Departments ...   \n",
       "248888  1 National Government Ministries, Departments ...   \n",
       "\n",
       "                     National Government Votes & Counties  \\\n",
       "39293   118 Ministry of East African Affairs, Commerce...   \n",
       "505287  110 Ministry of Environm,ent, Water and Natura...   \n",
       "395201                          107 The National Treasury   \n",
       "192100                          107 The National Treasury   \n",
       "248888                             108 Ministry of Health   \n",
       "\n",
       "                                                Sub Votes  \\\n",
       "39293         1185 State Department For Social Protection   \n",
       "505287  1109 Ministry of Water & Sanitation and Irriga...   \n",
       "395201                         1071 The National Treasury   \n",
       "192100                         1071 The National Treasury   \n",
       "248888                            1081 Ministry of Health   \n",
       "\n",
       "                                             Head/Dept  \\\n",
       "39293                          11850012 Cash Transfers   \n",
       "505287              11090022 Land Reclamation Services   \n",
       "395201           10710019 District Treasuries Services   \n",
       "192100           10710019 District Treasuries Services   \n",
       "248888  10810090 Kenya Expanded Programme Immunization   \n",
       "\n",
       "                                   SubHead/SubDept  \\\n",
       "39293   1185001202 Cash Transfers - Field Services   \n",
       "505287   1109002201 Land Reclamation Services - HQ   \n",
       "395201                     1071001901 Headquarters   \n",
       "192100                     1071001901 Headquarters   \n",
       "248888                     1081009001 Headquarters   \n",
       "\n",
       "       County Government Ministries National/County (Geo1)  \\\n",
       "39293                           NaN            02 Counties   \n",
       "505287                          NaN            01 National   \n",
       "395201                          NaN            02 Counties   \n",
       "192100                          NaN            02 Counties   \n",
       "248888                          NaN            01 National   \n",
       "\n",
       "            Counties (Geo2)  ...                   SOF2  \\\n",
       "39293     4610 Narok County  ...  00 Domestic Resources   \n",
       "505287     0000 Nation-Wide  ...  00 Domestic Resources   \n",
       "395201  4660 Kajiado County  ...  00 Domestic Resources   \n",
       "192100  4710 Kericho County  ...  00 Domestic Resources   \n",
       "248888     0000 Nation-Wide  ...  00 Domestic Resources   \n",
       "\n",
       "                           SOF3                        SOF4  \\\n",
       "39293   00001 Consolidated Fund  00001001 Exchequer ( GOK )   \n",
       "505287  00001 Consolidated Fund  00001001 Exchequer ( GOK )   \n",
       "395201  00001 Consolidated Fund  00001001 Exchequer ( GOK )   \n",
       "192100  00001 Consolidated Fund  00001001 Exchequer ( GOK )   \n",
       "248888  00001 Consolidated Fund  00001001 Exchequer ( GOK )   \n",
       "\n",
       "                                                   Sector  \\\n",
       "39293        09 Social Protection, Culture and Recreation   \n",
       "505287  10 Environment Protection, Water And Natural R...   \n",
       "395201  07 Public Administration And International Rel...   \n",
       "192100  07 Public Administration And International Rel...   \n",
       "248888                                          04 Health   \n",
       "\n",
       "                                                Programme  \\\n",
       "39293               0909000000 National Social Safety Net   \n",
       "505287         1014000000 Irrigation and Land Reclamation   \n",
       "395201             0718000000 Public Financial Management   \n",
       "192100             0718000000 Public Financial Management   \n",
       "248888  0401000000 Preventive, Promotive & Reproductiv...   \n",
       "\n",
       "                                            Sub-programme  \\\n",
       "39293   0909010000 Social Assistance to Vulnerable Groups   \n",
       "505287                        1014020000 Land Reclamation   \n",
       "395201                     0718040000 Accounting Services   \n",
       "192100                     0718040000 Accounting Services   \n",
       "248888  0401030000 Reproductive Maternal Neo-natal Chi...   \n",
       "\n",
       "       Initial Budget (Printed Estimate) Final Budget (Approved Estimate)  \\\n",
       "39293                                  0                          88326.0   \n",
       "505287                             22156                          11078.0   \n",
       "395201                                 0                            633.0   \n",
       "192100                                 0                          11681.0   \n",
       "248888                            900000                         900000.0   \n",
       "\n",
       "       Final Expenditure (Total Payment Comm.) Unnamed: 35  \n",
       "39293                                      0.0         NaN  \n",
       "505287                                     0.0         NaN  \n",
       "395201                                   633.0         NaN  \n",
       "192100                                 11681.0         NaN  \n",
       "248888                                460000.0         NaN  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9a94e5f-1a3c-4780-8bc3-f17914220c76",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that the data essentially consists of expenditure line items, tagged with columns representing an institutional hierarcy, geographic hierarchy, a program hierqarchy and finaly it includes budgeted, revised and executed amounts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33f0fef9-7d3f-4850-997b-7ab69eac74e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PySpark\n",
    "PySpark is the Python interface for [Apache Spark](https://spark.apache.org/), a distributed computing framework used for processing large datasets. PySpark allows users to perform transformations at scale using the familiar Python syntax, while Spark handles the underlying distributed computation efficiently.\n",
    "\n",
    "In these code snippets, the `spark` variable is a pre-configured SparkSession, which is the entry point for interacting with Spark and is available by default in databricks. It enables users to load data, run queries, and perform transformations across a cluster of machines, making it easy to process large datasets in parallel. This SparkSession also manages connections to the underlying cluster resources.\n",
    "\n",
    "We can read data from the datalake (for which we have permission) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.table('prd_corpdata.dm_operation_gold.project')\n",
    "df = spark_df.toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have read the data (which you are already familiar with from the previous session) into a **spark dataframe**. Then, if we prefer to work with Pandas we can convert the spark dataframe into a **python dataframe** with the .to_Pandas() method. \n",
    "\n",
    "So, accessing and reading data from the data lake only takes a line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77df7c63-fb72-4a8c-a856-f0d210df7d45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8a4994f-6af0-4510-9c98-f2ca3f214e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this tutorial we follow the medallion architecture which is an architecture type where we have a series of data layers that denote the quality of data stored (in the data lake).This architecture ensures consistency and isolation data passes through multiple layers of validations and transformations before being stored. The terms bronze (raw), silver (validated), and gold (enriched) refer to the quality of data at each respective layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8800077-7a8c-41cf-93e8-b7a6345eb3c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The flow diagram below shows the flow of data through these layers, and we will illustrate this model using the Kenya BOOST data. \n",
    "\n",
    "These datasets at various stages are written to tables in the **Delta format** to the data lake. The Delta format allows for scalability, consistency during concurrent writes, data versioning and schea enforcement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056d5129-b68a-4cab-b053-f9cb9c958a10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "![Medallion Flowchart](assets/medallion_flow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99eaa168-ebe2-4de2-b41d-6fe95c0da1cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bronze\n",
    "\n",
    "The code snippet below demonstrates the bronze stage of a data pipeline following the medallion architecture. \n",
    "\n",
    "The full script is saved as bronze.py in this project folder. We will not run the following snippet but will illustrate the working of the code. \n",
    "\n",
    "\n",
    "\n",
    "Raw microdata is ingested from CSV files, cleaned by standardizing column names, and then stored in the bronze table using the optimized Delta format. Additionally, a database is created in Databricks SQL to manage the tables effectively. This step prepares the raw data for further processing in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84e258dd-ed46-4cb3-8de4-3943e5fccd7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = (spark.read\n",
    "             .format(\"csv\")\n",
    "             .options(**CSV_READ_OPTIONS)\n",
    "             .option(\"inferSchema\", \"true\")\n",
    "             .load(Data_DIR))\n",
    "\n",
    "# Clean column names by replacing spaces and special characters\n",
    "for old_col_name in bronze_df.columns:\n",
    "    new_col_name = old_col_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\",\", \"\")\n",
    "    bronze_df = bronze_df.withColumnRenamed(old_col_name, new_col_name)\n",
    "\n",
    "\n",
    "# Create the database in Databricks SQL\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "\n",
    "# Save to bronze table (Databricks Delta format for optimization)\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.kenya_bronze\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab76e022-55ae-47dd-b5d8-b55fc139c424",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Silver\n",
    "\n",
    "In the [silver](silver.ipynb) stage, we read the data produced in the bronze stage and transform and refine the data. \n",
    "\n",
    "In this stage, we append columns constructed from existing columns in the bronze table using the *.withColumn()* method. We construct columns representing where money is spent (geo1), who wwas responsible for the spending (admin0 and admin1), along with other columns like the functional category of the spending (func and func_sub). We also filter the data using the *.filter()* method to retain only the data related to financial execution.\n",
    "\n",
    "We use a helper function to check if a line item needs to be classified as 'Recreation and Culture' or 'Housing' by checking for the presence of certain key terms within the *Programme_pro2* column. The actual conditions for classifying the line items is domain dependent but the transformations used here are illustrative of the typical transformations that one encounters in general workflows.\n",
    "\n",
    "Finally, we write the transformed and enriched dataset to a delta table called silver to the datalake.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36aa9a35-cbb5-4ef3-a483-61f040b80461",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_df = (spark.read.table(f\"{database_name}.kenya_bronze\")\n",
    "    # Here we use the filter method with a specific condition on the Class column to retain only the rows we need\n",
    "     .filter(~col('Class').isin('2 Revenue', '4 Funds & Deposits (BTL)'))\n",
    "    \n",
    "        #... Other filter conditions follow\n",
    "    \n",
    "     # Here we use the withColumn method to define a new column -- admin2 using some condition on other existing columns   \n",
    "     # We use a specific condition on the column called National_Government_Votes_&_Counties_adm2 and modifying the entries there\n",
    "     # The regex_replace method helps manipulate the strings in the previously mentioned column to modify the string when a certain pattern is seen\n",
    "    .withColumn('admin2', regexp_replace(col(\"National_Government_Votes_&_Counties_adm2\"), '^[0-9\\\\s]*', ''))\n",
    "    \n",
    "         #... multiple other .withColumn conditions follow where we define some new columns\n",
    "    \n",
    "    # Here again we modify the column called Year and then construct the column year. \n",
    "    # We also use the cast method to set its data type\n",
    "    .withColumn('year', concat(lit('20'), substring(col('Year'), -2, 2)).cast('int'))\n",
    "             \n",
    "    # Here again we use the withColumn method to construct a column called func_sub but with a more complicated set of rules\n",
    "    .withColumn('func_sub',\n",
    "                when((col('Sector_prog1').startswith('06') & \n",
    "                     (col('National_Government_Votes_&_Counties_adm2').startswith('102') |\n",
    "                      col('National_Government_Votes_&_Counties_adm2').startswith('210') |\n",
    "                      col('National_Government_Votes_&_Counties_adm2').startswith('215'))),\n",
    "                     \"public safety\")\n",
    "                .when(col('Sector_prog1').startswith('06'), \"judiciary\")\n",
    "                .when(col('Programme_pro2').startswith('0401'), 'primary and secondary health')\n",
    "                .when(col('Programme_pro2').startswith('0402'), 'tertiary and quaternary health')\n",
    "                .when((col('Sector_prog1').startswith('05') & \n",
    "                      col('Programme_pro2').startswith('0501')), 'primary education')\n",
    "                .otherwise('General public services'))\n",
    "        #... Other modifications follow to construct and produce the new columns\n",
    "             \n",
    "# Save to silver table\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.kenya_silver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99f5cfdc-4921-4780-8fc0-83f0fb259674",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Gold\n",
    "\n",
    "In the gold stage of the pipeline (gold.py in the project folder), we finalize the dataset for analysis. The focus here is on filtering, selecting key columns, and ensuring the data is structured for consumption.\n",
    "\n",
    "This stage refines the data into a ready-to-use form. We retain columns deemed necessary for the analysis, as the remaining columns are available in the silver table if required later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f03058a-35da-43b5-a8c6-c9bd0ac209c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_df = (spark.read.table(f\"{database_name}.kenya_silver\")\n",
    "    .filter(col('year') != 2015) # Exclude 2015 data\n",
    "    .withColumn('country_name', lit('Kenya'))\n",
    "    .select('country_name',\n",
    "            'year',\n",
    "            col('Initial_Budget_Printed_Estimate').alias('approved').cast(DoubleType()),\n",
    "            col('Final_Budget_Approved_Estimate').alias('revised'),\n",
    "            col('`Final_Expenditure_Total_Payment_Comm.`').alias('executed'),\n",
    "            'admin0',\n",
    "            'admin1',\n",
    "            'admin2',\n",
    "            'geo1',\n",
    "            'is_foreign',\n",
    "            'func_sub'\n",
    "           )\n",
    ")\n",
    "\n",
    "# Save to gold table\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.kenya_gold\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b285fd1-0002-4ed8-8736-26165242f9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregation and appending additional columns\n",
    "\n",
    "In the above steps we cleaned the original source BOOST Kenya data through multiple stages. In addition to this, we might need external datasources to append to this data. In our example, we want to add information about per capita spending.\n",
    "\n",
    "In this case, we want to get the subnational population from an external source (an API) and then aggregate our kenya_gold data to get a table containing the spending by *geo1* region and year. Since we referenced this very dataset [above](#do-we-already use-data-pipelines?-or-are-they-completely-new?), we will not go over the code again but it can be accessed [here](subnational_population.ipynb). Then we will append a new column called per_capita_spending to this aggregated table. We finally save this table so that these descriptive figures are readily available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a91b14c-a597-4501-b2f9-2f915189726c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Aggregation\n",
    "Finally, to get the aggregation table we read the kenya_gold table and kenya_subnational_population tables. Wre aggregate the kenya_gold table along the year, geo1 and func columns and join the population column to the aggregated table\n",
    "\n",
    "We compute per_capita_spending and append this column to the aggregated table above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "233863f5-14ef-448d-b89e-d532c64a160c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pop_df = spark.read.table(f\"{database_name}.kenya_subnational_population\").alias(\"pop_df\")\n",
    "\n",
    "agg_df = (spark.read.table(f\"{database_name}.kenya_gold\")\n",
    "    .groupBy(\"country_name\", \"year\", \"geo1\", \"func\",)\n",
    "    .agg(sum(\"executed\").alias(\"expenditure\"))\n",
    ").alias(\"agg_df\")\n",
    "\n",
    "agg_with_pop_df = (agg_df\n",
    "    .join(pop_df, \n",
    "          (agg_df.country_name == pop_df.country_name) & \n",
    "          (agg_df.geo1 == pop_df.adm1_name) & \n",
    "          (agg_df.year == pop_df.year), \n",
    "          \"inner\")\n",
    "    .select(\"agg_df.country_name\", \"agg_df.year\", \"agg_df.geo1\", \"agg_df.func\", \"agg_df.expenditure\", \"pop_df.population\")\n",
    ")\n",
    "\n",
    "agg_with_pop_df = agg_with_pop_df.withColumn(\n",
    "    'per_capita_spending',\n",
    "    (col('expenditure') / col('population')).alias('per_capita_spending')\n",
    ")\n",
    "\n",
    "agg_with_pop_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{database_name}.kenya_func_geo1_agg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d122eee1-a4a3-4526-9d77-fdd05afa0238",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Orchestration\n",
    "\n",
    "Orchestration refers to the process of coordinating and automating the various stages of the pipeline—such as data extraction, transformation, and loading—to ensure smooth, sequential execution. This is equivalent to *connecting the individual jobs* so that they run in an organized manner keeping track of the dependencies between the jobs. In PySpark, this involves managing the flow of tasks.\n",
    "\n",
    "Orchestration tools help **schedule** and manage these stages, ensuring that dependencies between tasks are handled correctly and that the pipeline runs efficiently. This ensures data moves through the pipeline in an automated manner, handling errors and retries where necessary.\n",
    "\n",
    "We will use the databricks built-in *Workflows* for managing these dependencies and sceduling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ef6f62-8d24-4c15-a221-95d5be0a1dba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Medallion Flowchart](assets/kenya_workflow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b4c155b-f121-4555-b56d-1c5e36277dee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The above flow diagram of the jobs and dependencies between the jobs is obtained by adding the jobs and then adding the other jobs that the job at hand depends on.\n",
    "\n",
    "Note that for silver.py script to run, we need the bronze.py to run since we need to read the bronze table in silver.py. This shows the dependence of silver.py on bronze.py. Similarly, we can get all the dependencies as shown in the flow diagram above. This forms a directed acyclic graph (DAG) which ensures that no circular dependence is possible. \n",
    "\n",
    "The above diagram shows the successful (manual) run of the workflow -- the success of each job indicated by the green line at the top edge of each box. This also makes debugging easier since we can identify the failure of a job within this workflow.\n",
    "\n",
    "\n",
    "This saves the user the difficulty of running these scripts **manually**. In addition to this, the workflow can be scheduled so that the entire collection runs in the correct order. This scheduling can be done either manually (i.e., run the workflow manually-- not the individual jobs), or set to run at specific intervals, or triggered when there is a new file added to a specific folder for instance, which makes the **automation** of this workflow extremely flexible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed8a64f-696e-448c-bd05-548fdcab7ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### References\n",
    "\n",
    "1. [databricks reference for data pipelines](https://docs.databricks.com/en/getting-started/data-pipeline-get-started.html)\n",
    "2. [PySpark basics](https://docs.databricks.com/en/pyspark/basics.html)\n",
    "3. General notes on [orchestration](https://www.datacamp.com/blog/introduction-to-data-orchestration-process-and-benefits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38684896-07c9-4262-bd04-058da2eb6173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "intro-data-pipelines",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
