{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeee32c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/worldbank/dec-python-course/blob/main/2-advanced-topics/text-analysis/intro-text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23677227",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "Text analysis is the process of extracting meaningful information from text data, uncovering insights that would otherwise remain buried under text corpora.\n",
    "\n",
    "This session is an **introduction** to text analysis. We'll be covering the following topics:\n",
    "\n",
    "1. Regex and character patterns in text data\n",
    "1. Text data pre-processing\n",
    "1. Counting words\n",
    "1. Text classification\n",
    "\n",
    "The session assumes previous knowledge of Python and Pandas, and some knowledge of data visualization using seaborn.\n",
    "\n",
    "We'll use the following libraries in this notebook:\n",
    "\n",
    "- **pandas** for dataframe operations\n",
    "- **re** for regular expressions\n",
    "- **spacy** for text data preparation\n",
    "- **seaborn** for data visualization\n",
    "- **sklearn** for data classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11190e",
   "metadata": {},
   "source": [
    "## (some) Data exploration\n",
    "\n",
    "We'll start by getting familiarized with our dataset. We'll use a structured tabular dataset of working papers obtained from the WB Documents API.\n",
    "\n",
    "Run the following line to load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/worldbank/dec-python-course/main/2-advanced-topics/text-analysis/data/papers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85790a75",
   "metadata": {},
   "source": [
    "The data is a corpus of working papers from the WB Policy Research Working Paper series. For each paper, we have:\n",
    "\n",
    "- A paper identifier\n",
    "- The Title\n",
    "- Two URLs\n",
    "- The topics of the paper, separated by commas\n",
    "- An abstract\n",
    "- A text\n",
    "\n",
    "Let's take a closer look at the columns `url`, `url_text`, and `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697c1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea4a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6966058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716b1b6",
   "metadata": {},
   "source": [
    "`url` contains the paper URL, `url_text` is the URL to actual text content, and `text` is the text of the paper.\n",
    "\n",
    "Now that we know what the data is about, we can start planning what to do with it. In general, all the tasks we'll do are about data augmentation, and basic descriptive and classification tasks. This is a summary of what we'll do:\n",
    "\n",
    "1. Generate new features (columns) based on the text\n",
    "1. Count the words and most used words\n",
    "1. Build a topic classifier from our corpus\n",
    "\n",
    "For the first task, we'll augment the dataset using existing patterns in the text.\n",
    "\n",
    "## Patterns\n",
    "\n",
    "Let's take another look at the text. This time we'll use the function `print()`, so that space characters are properly rendered and the text is easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52cb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8be813",
   "metadata": {},
   "source": [
    "Note that there are a number of information elements that seem to follow some patterns in the text:\n",
    "\n",
    "- The WP number is in the last sequence of non-space characters in the first line\n",
    "- The authors' names is a series of contiguous lines after the paper title\n",
    "- Abstract: lines after the word \"Abstract\" in the beginnning of the text. All of them seem to have a big space in the middle of the sentence\n",
    "- Keywords: separated by a semi-colon in a line that starts with \"Keywords\"\n",
    "- JEL Codes: an uppercase caracter followed by two numbers, separated by commas\n",
    "- Authors emails: non-space sequence of characters with \"at\" sign (\"@\") and ending in \".org\", \".com\"\n",
    "- Bibliography elements: last lines of the text\n",
    "\n",
    "We're going to take advantage of the patterns of JEL codes to extract them in a new column and augment our original dataframe. We'll use regular expressions for this.\n",
    "\n",
    "**Important:** We're only checking one observation (the first) when inferring these patterns. If you want to augment an entire dataframe and not a single observation, you'd have to make sure the same pattern exists in the rest of the texts of your corpus. We'll take it for granted in this session for the sake of time, but you should note that manually exploring different observations of your corpus is needed to infer possible patterns in your texts.\n",
    "\n",
    "### Regular expressions\n",
    "\n",
    "In programming, regular expressions are sequences of characters that match a pattern in text. A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The ID number of participant 1 is 30551. They were born on July 01, 1996. Participant 2 has ID 71098.'\n",
    "\n",
    "# Pattern for capturing IDs in this text: sequences of five number characters:\n",
    "pattern = '\\d{5}'\n",
    "\n",
    "# Capturing IDs\n",
    "ids = re.findall(pattern, text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0e4b",
   "metadata": {},
   "source": [
    "Some notes about this code:\n",
    "- `\\d` is a wildcard that represents one number (0-9). This is also the same as `[0-9]`\n",
    "- `{5}` means that the previous character in the pattern is repeated five times\n",
    "- A variation of this pattern could be `\\d{4}`, which could be used to capture years. This would have returned a list with `1996` in the example above\n",
    "\n",
    "In regex, there is a wildcard for almost everything. Some examples:\n",
    "\n",
    "- Character wildcards:\n",
    "    + `\\d` --> digits (0-9)\n",
    "    + `\\W` --> any word character (uppercase and lowercase a-z, digits, and underscore (\"_\") )\n",
    "    + `\\n` --> newline characters\n",
    "    + `\\s` --> whitespace characters, including newline\n",
    "    + `.` --> any character except newline\n",
    "- Character repetition:\n",
    "    + `{a}` --> the previous character, repeated \"a\" times\n",
    "    + `{a,b}` --> the previous character, repeated between \"a\" and \"b\" times\n",
    "    + `*` --> the previous character, repeated zero or more times\n",
    "    + `+` --> the previous character, repeated one or more times\n",
    "    \n",
    "Regex can match any pattern we can possibly imagine. However, working with regex can be complex for starters. For the purpose of this session, we've introduced regex so you know it exists and can be used to augment datasets containing corpus of documents. Don't worry for now if you still didn't grasp well how the patterns work, but if you're interested in learning more about rege, we recommend the following resources:\n",
    "\n",
    "- A nice regex tutorial is [here](https://regexone.com/)\n",
    "- A great regez visualizer tool is [here](https://jex.im/regulex/#!flags=&re=www%5C.%5Ba-zA-Z0-9-%5D%2B%5C.(%3F%3Acom%7Cnet%7Corg))\n",
    "\n",
    "### Extracting information using patterns\n",
    "\n",
    "Remember we said that JEL codes in the text looked like a pattern of one uppercase letters followed by two digits? We'll use this to extract the JEL codes of each paper in a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '[A-Z]\\d{2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af17b44",
   "metadata": {},
   "source": [
    "This pattern captures one uppercase alphabetic character (`[A-Z]`), followed by one digit repeated two times (`\\d{2}`).\n",
    "\n",
    "Now we'll define a helper function that looks for this pattern in a text and returns all captures in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf537283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_jel(text):\n",
    "    \n",
    "    pattern = '[A-Z]\\d{2}'\n",
    "    result = re.findall(pattern, text)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ae412c",
   "metadata": {},
   "source": [
    "Lastly, we'll map this function using Pandas' `apply()` method to create a new column in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c373cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['jel'] = df['text'].apply(capture_jel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656168f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44f045d",
   "metadata": {},
   "source": [
    "Now we have augmented our dataset. Great!\n",
    "\n",
    "For the next part of the session, we'll start properly analyzing and getting insights from the text contents. The final result of the next part will be a count of the most used words in each text and we'll also count the total number of words in each text during the process.\n",
    "\n",
    "## Text data pre-processing\n",
    "\n",
    "Before we start, we need to think of the following:\n",
    "\n",
    "- Our texts are in a very raw state. Shouldn't we \"clean\" them a bit before counting words?\n",
    "- Using regex to capture words so we can count them sounds possible, but perhaps there is an easier way?\n",
    "- Texts in English usually repeat a lot words that are not very insightful about the content, such as prepositions or pronouns. Can we get rid of some of them before the word count?\n",
    "- Lastly, shouldn't we count in the same category words that are not exactly the same but have a very similar meaning? for example:\n",
    "    + different conjugations of the same verb\n",
    "    + singular and plural forms of the same noun\n",
    "    \n",
    "The answer to all of these questions is Yes. We'll do this in the data pre-processing. Data pre-processing in text analysis is extremely important. Omitting pre-processing will give you different results in text analysis tasks.\n",
    "\n",
    "Data pre-processing can consist of multiple tasks. We'll apply the following for our corpus:\n",
    "\n",
    "- Transform to lowercase\n",
    "- Tokenization: transform texts into lists of words\n",
    "- Remove stop words (words that are not very insightful, such as prepositions)\n",
    "- Lemmatization: transform different forms of words into a common word that conveys a similar meaning. This is useful to \"normalize\" conjugations of verbs or plural forms of words\n",
    "\n",
    "Fortunately, there is a very useful Python library we can use for this: [spaCy](https://spacy.io/). SpaCy makes available pre-existing NLP models that tokenize, lemmatize, and detect stop words and non-word characters (such as digits or punctuation), so we can easily transform a text into a list of \"meaningful\" lemmatized words that we can use for word counts.\n",
    "\n",
    "### Working with spaCy\n",
    "\n",
    "First we need to install spaCy. Uncomment the line below, run it, and then comment it again with `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b7d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a9ba4",
   "metadata": {},
   "source": [
    "Now we need to **download** spaCy's NLP model. Uncomment the line below, run it only once, and then comment it out again to make sure you won't run it again accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdb6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639db72e",
   "metadata": {},
   "source": [
    "Now we **load** the model so it's available in this Python notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482bce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8608234",
   "metadata": {},
   "source": [
    "Then, we'll build a function that:\n",
    "\n",
    "1. Reads a text\n",
    "1. Transforms it to lowercase\n",
    "1. Loads it into the model\n",
    "1. For each word, obtains the lemmatized versions of words that are not:\n",
    "    - Stop words\n",
    "    - Punctuation\n",
    "    - Numbers\n",
    "    - Spaces\n",
    "1. Finally, the words returns a list of the lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6694c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenization_normalization(text):\n",
    "    \n",
    "    text = text.lower() # lowercase\n",
    "    doc = nlp(text)     # loading text into model\n",
    "\n",
    "    words_normalized = []\n",
    "    for word in doc:\n",
    "        if word.text != '\\n' \\\n",
    "        and not word.is_stop \\\n",
    "        and not word.is_punct \\\n",
    "        and not word.like_num \\\n",
    "        and len(word.text.strip()) > 2:\n",
    "            word_lemmatized = str(word.lemma_)\n",
    "            words_normalized.append(word_lemmatized)\n",
    "    \n",
    "    return words_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e9f90",
   "metadata": {},
   "source": [
    "To get a better idea of what the function does, let's take a look at the result for one paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'][10]\n",
    "doc_tokenized = word_tokenization_normalization(df['text'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990522a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a18c",
   "metadata": {},
   "source": [
    "The result is a list of normalized words for the text.\n",
    "\n",
    "You might have also noticed that this takes some time to run. To avoid having to wait, we'll apply the function to tokenize and normalize only the **abstracts**. We'll again use the Pandas method `apply()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee78234",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_tokenized'] = df['abstract'].apply(word_tokenization_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb35728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829719f",
   "metadata": {},
   "source": [
    "The downside of having applied the tokenization and normalization on the abstracts is that we might not have abstracts long enough to make word repetition very insightful. In a non-training setting, we should have used the full texts, leave the code running while we do other things or go for coffee, and come back and work with the results once the code finishes.\n",
    "\n",
    "## Counting words\n",
    "\n",
    "Now that the texts are normalized, we can count words! We'll do two things:\n",
    "\n",
    "1. Generate a column with the number of words\n",
    "1. Generate a column with a dictionary where each word is a key and the number of times are the key's values. This will look like `{'word1': n1, 'word2': n2, ...}`\n",
    "\n",
    "For the first task, we can directly create a new column with the result in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df410125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_words_abstract'] = df['abstract_tokenized'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30076b94",
   "metadata": {},
   "source": [
    "Just out of curiosity, let's pause for a minute to see the distribution in the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3333cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this line if you don't have seaborn:\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f89e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=df, x='n_words_abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bc464a",
   "metadata": {},
   "source": [
    "For the second task, we need to generate a helper function that generates the dictionary from each tokenized abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45512e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counts(tokenized_text):\n",
    "    \n",
    "    count = {}\n",
    "    \n",
    "    for word in tokenized_text:\n",
    "        if word in count:\n",
    "            count[word] += 1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0038b4",
   "metadata": {},
   "source": [
    "We'll first apply the function to only one text to make sure the result looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_tokenized = df['abstract_tokenized'][42]\n",
    "count = word_counts(abstract_tokenized)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0e55e",
   "metadata": {},
   "source": [
    "This looks interesting, but it's not very meaningful unless we spend some time looking at the result. We'll transform this into a barplot for easier interpretation but only keeping the words with more than 2 counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_trimmed = {}\n",
    "for word, value in count.items():\n",
    "    if value > 2:\n",
    "        count_trimmed[word] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29e0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(count_trimmed, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e3fa48",
   "metadata": {},
   "source": [
    "Now we'll apply the function `word_counts()` to all the abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b94201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract_word_count'] = df['abstract_tokenized'].apply(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f443f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65ecbc",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "\n",
    "For the last part of the session, we'll do a simple text classification example. We're calling this \"simple\" because there are now available very fancy and state-of-the-art text classification techniques for text, but that are not suitable for a 90-minute session. You can check the link listed below about LLMs if you want to explore more about these.\n",
    "\n",
    "Simply put, text classification consists of assigning a text to a pre-defined group. If you're familiar with machine learning, this is exactly a supervised machine learning classification task. For our exercise, the pre-defined groups will be the first topic of the column `topics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e26877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_topic'] = df['topics'].apply(lambda x: x.split(',')[0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a5119",
   "metadata": {},
   "source": [
    "Now we'll tabulate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12294ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['first_topic'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b815435",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8432404a",
   "metadata": {},
   "source": [
    "There are 198 topics for a total of 399 papers (!), which means that a lot of topics have only one or two papers. We'll keep only topics that have at least five papers so that there is at least some observations in each topic to build a classifier. This will reduce the size of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11aa2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_keep = df['first_topic'].value_counts()[df['first_topic'].value_counts() >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5234b7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_to_keep.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e4af5",
   "metadata": {},
   "source": [
    "Our resulting dataframe will have only have 148 observations. This is not enough to generate a good classifier but we'll still go ahead and use it for the exercise as an example of the application of the text classification method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f330472",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['first_topic'].isin(topics_to_keep.index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10809f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1a410",
   "metadata": {},
   "source": [
    "### Text encoding\n",
    "\n",
    "Our classifier will be built (trained) using the tokenized and normalized abstracts. However, we need first to convert them into numbers so a classifier con work with them. This operation is called **encoding**.\n",
    "\n",
    "There are several ways of encoding texts. We'll use term-frequency inverse-document frequency (TF-IDF). TF-IDF transforms a collection of words into a numeric vector where each word has a weight. It gives high weight to words that show up a lot in a given document, but rarely across documents in the corpus (so they are more distinctive for the document only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66c8f1",
   "metadata": {},
   "source": [
    "We'll start by installing the library we'll use for the encoding and text classification: scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below for the installation:\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the encoder\n",
    "corpus = list(df2['abstract_tokenized'].apply(lambda x: ' '.join(x)))\n",
    "encoder = TfidfVectorizer(stop_words = ['paper'], max_features=1000)\n",
    "vectors = encoder.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c418495",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61306214",
   "metadata": {},
   "source": [
    "For an easier understanding of the text encoding, we'll transform this back into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e88c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_encoded = encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eedd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_data = vectors.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data=vectors_data, columns=words_encoded)\n",
    "df_tfidf.insert(0, 'title', df2['title']) # inserting the paper title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3348e7",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "Now that our data is ready, we can train a classifier with it. We'll use a multinomial Naive Bayes classifier in this example, but other types of classifiers are available in the library we're using (scikit learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d638f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781b2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afdf8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_tfidf.drop(columns = 'title')\n",
    "y = df2['first_topic']\n",
    "classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762fdd8",
   "metadata": {},
   "source": [
    "After this, `classifier` has been trained with the data in `x` to know which patterns in it produce the results in `y`.\n",
    "\n",
    "### Classification\n",
    "\n",
    "Now we'll classify our texts with the classifier we trained. Given that it was trained with encoded normalized words, the input for any classification should also be encoded normalized words. We'll use our same data of `tf_idf` to produce a classification and will compare it the actual true values to have a sense of how well this classifier performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = df2[['title', 'first_topic']]\n",
    "df_predictions['predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b32df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['correct'] = False\n",
    "df_predictions.loc[df_predictions['first_topic'] == df_predictions['predictions'], 'correct'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a216d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions['correct'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8218cd",
   "metadata": {},
   "source": [
    "Some notes on this result:\n",
    "\n",
    "- Our classifier is only 50% accurate. This is not a good performance but we also had to work with very small data that we can manage in a short training session. In a real setting, you should have ideally with 1,000+ observations and different types of classifiers.\n",
    "- We are using our classifier on the same data we used for training it. In a real setting, this is a very bad practice as it will likely lead to overfitting: producing a classifier that works well for the data it was trained on but can't really generalize for out-of-sample cases. The way you avoid this is by separating your data in a training dataset and a test dataset. Then you use the training set for training and the test set for evaluating its performance.\n",
    "- You probably noticed that the dataframe `df_tfidf` is a sparse matrix. Sparse matrix are common results of TF-IDF encoding. You can use principal component analysis to reduce the matrix into a few meaningful components and work with that. This will make the computation easier.\n",
    "- Moreover, you can augment the PCA vectors or TF-IDF matrix with other data that will probably have predicting power for the variable we classify. Remember we extracted the JEL topics before? those are probably good predictors in this case.\n",
    "\n",
    "## Final notes\n",
    "\n",
    "### Other languages\n",
    "\n",
    "These exercises used a corpus in English. However, the principles for working with other languages are just the same for all of these text classification tasks. SpaCy has NLP models in other languages available, you can check them [here](https://spacy.io/usage/models).\n",
    "\n",
    "### Other text analysis tasks\n",
    "\n",
    "This was an overview of possibly the simplest text analysis tasks. Other tasks are:\n",
    "\n",
    "- Named entity recognition: detecting mentions of a meaningful entity (places, names of people, dates, etc) in texts\n",
    "- Cluster classification and topic modeling: classifying texts into groups based on their similarity\n",
    "- Vector spaces and word embeddings: transforming texts or words into vectors of \"meanings\". You can then work with them for other tasks, such as compare the proximity of texts based on meanings\n",
    "- Generative AI with texts: generating texts based on prompts or previous text.\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "\n",
    "We didn't cover LLMs because they're not part of an introductory session. If you're more interested in learning about them, we recommend these readings:\n",
    "\n",
    "- BERT was the first (or at least one of the first?) LLM publicly released. This article explains well how it works: [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "- This is a tutorial of how to work with BERT to fine-tune it for specific NLP/text analysis tasks: [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
