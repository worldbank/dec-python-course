{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/worldbank/dec-python-course/blob/main/3-other-languages/Python-para-ciencia-de-datos/Sesion%204a%20-%20Estructuracion%20de%20datos%20de%20texto%20no%20estructurados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23677227",
   "metadata": {},
   "source": [
    "# Introducción al Análisis y Mineria de Textos\n",
    "\n",
    "Análisis y mineria de texto es el proceso de extraer información significativa a partir de datos textuales, revelando ideas que de otro modo permanecerían ocultas en grandes volumenes de texto. El termino \"texto\" aqui se refiere a cualquier conjunto de caracteres: podria desde libros enteros hasta una sola oracion o palabra.\n",
    "\n",
    "Esta sesión y la siguiente son una **introducción** al análisis de texto. Cubriremos los siguientes temas:\n",
    "\n",
    "1. Estructuracion de datos de texto no estructurados\n",
    "    1. Reconocimiento de caracteres desde documentos en PDF\n",
    "1. Limpieza y preparacion de datos de texto\n",
    "    1. Expresiones regulares y patrones de caracteres en datos de texto  \n",
    "    1. Preprocesamiento de datos textuales  \n",
    "1. Analisis descriptivo de datos de textos\n",
    "    1. Conteo de palabras\n",
    "    1. Análisis de sentimiento  \n",
    "1. Clasificación de textos\n",
    "\n",
    "Veremos los dos primeros puntos en la sesion de hoy y los dos ultimos puntos manana.\n",
    "\n",
    "Esta sesión asume conocimientos previos de Python y Pandas, así como cierto conocimiento de visualización de datos usando seaborn. Todo lo que cubrimos en las tres primeras sesiones de este taller es suficiente base para continuar con analisis de textos.\n",
    "\n",
    "Usaremos las siguientes bibliotecas en este notebook:\n",
    "\n",
    "- **pdfminer** para \"leer\" archivos PDF y extraer su contenido en textos\n",
    "- **pandas** para operaciones con dataframes  \n",
    "- **re** para expresiones regulares  \n",
    "- **spacy** para procesamiento de datos textuales\n",
    "\n",
    "# Sesion 4 - Estructuracion de datos de texto y preparacion de datos de texto\n",
    "\n",
    "# 1. Estructuracion de datos de texto no estructurados\n",
    "\n",
    "Los datos de en volumenes raramente tienen una estructura predeterminada. En muchos casos, estos vienen de archivos individuales que contienen textos. Por ejemplo, estos pueden ser un folder con decenas, cientos o miles de archivos en formato PDF, Word o `.txt`.\n",
    "\n",
    "Dar una estructura a estos archivos requiere que evaluemos cual es la mejor forma que una tabla de datos para cierto volumen de textos puede adquirir. Por ejemplo, al trabajar con un folder con cientos de archivos PDF de texto, podemos estructurar la tabla para que tenga una fila por documento, una fila fila por parrafo, una fila por oracion o incluso una fila por palabra.\n",
    "\n",
    "Para el siguiente ejemplo que usaremos en la primera mitad de esta sesion, partiremos de un volumen de documentos publicos del Banco Mundial. Estos documentos corresponden a algunos de los **reportes publicos que el Banco Mundial ha producido en Espanol sobre Republica Dominicana desde 2010 a 2024**.\n",
    "\n",
    "Estos documentos estan en la carpeta `docs/` y son todos archivos en PDF. La imagen debajo es una captura de pantalla del contenido de la carpeta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda85052",
   "metadata": {},
   "source": [
    "<img src=\"img/docs.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c77205",
   "metadata": {},
   "source": [
    "## 1.1 Reconocimiento optico de caracteres desde documentos en PDF\n",
    "\n",
    "El reconocimiento optico de caracteres (*Optical Character Recognition*--OCR por sus siglas en ingles) es una operacion comun en analisis de textos. Consiste en transformar textos que estan en algun documento (PDF or Word, por ejemplo) o en una imagen en un formato para texto que pueda ser operado por un lenguaje de programacion, por ejemplo en una string en Python.\n",
    "\n",
    "En este ejemplo, usaremos el paquete `pdfminer` para reconocer los caracteres de estos archivos PDF. Ten en cuenta que tambien es posible reconocer textos de archivos en Word o de imagenes a strings en Python.\n",
    "\n",
    "- [Mira aqui](https://github.com/microsoft/Simplify-Docx) un ejemplo (en ingles) para transformar archivos de Word a texto en Python\n",
    "- [Aqui](https://medium.com/do-it-with-code/extract-text-from-images-using-python-ocr-dc7092adf9a8) un ejemplo (tambien en ingles) para transformar imagenes con textos a strings en Python\n",
    "\n",
    "En general, existen muchas soluciones con metodos que logran resultados aceptables o buenos para reconocer caracteres de documentos o caracteres producidos con computadoras. Sin embargo, **reconocer escritura a mano**  es un proceso mucho mas complicado para el cual no existen soluciones gratuitas predeterminadas que funcionen bien en todos los casos. El ejemplo que veremos en esta sesion y los dos links del parrafo anterior posiblemente no logren buenos resultados para caracteres escritos a mano.\n",
    "\n",
    "Ahora continuaremos importando los paquetes necesarios para \"leer\" un PDF a texto en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af623657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala PDF miner si no lo tienes\n",
    "# !pip install pdfminer\n",
    "\n",
    "# Modulos de pdfminer para leer PDF\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "# Paquetes para trabajar con directorios\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f985e",
   "metadata": {},
   "source": [
    "Empezaremos creando una lista con la ubicacion de todos los documentos que queremos leer desde PDF. Estos estan en la carpeta `doc/`. Para esto, vamos a usar el paquete `os` que nos permite interactuar con folderes para trabajar con archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder cuyos archivos queremos incluir en la lista\n",
    "folder = 'docs/'\n",
    "\n",
    "# Definiendo una lista vacia para agregar la ruta de los archivos PDF\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538c9e6",
   "metadata": {},
   "source": [
    "El siguiente loop explora todos los archivos en `folder` y anade a la lista `docs` aquellos que tienen la extension `.pdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle a traves de los archivos en \"folder\"\n",
    "for archivo in os.listdir(folder):\n",
    "    \n",
    "    if archivo.endswith(\".pdf\"):             # si el archivo es un PDF, continuamos\n",
    "        \n",
    "        doc = os.path.join(folder, archivo)  # os.path.join une un nombre de carpeta y archivo para dar una ruta completa\n",
    "        print(f'Documento: {doc}')\n",
    "        docs.append(doc)                     # anadimos el archivo a la lista docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518028e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff55d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f50d9",
   "metadata": {},
   "source": [
    "`docs` es ahora una lista con las rutas a los documentos PDF. Ahora vamos a iterar a traves de la lista para leerlos usando la funcion `texto_PDF()` que definiremos en el siguiente bloque.\n",
    "\n",
    "Esta funcion es bastante complicada, pero funciona bien para casi todos los casos en que cualquier usuario tendria que leer un PDF. No es necesario entender todo lo que contiene la funcion dado que algunos de estos comandos son bastante especializados. Para nuestro uso, no la modificaremos y la vamos a utilizar tal como esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52af512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_PDF(pdfFile):\n",
    "    \n",
    "    # Basado en codigo de http://stackoverflow.com/a/20905381/4955164\n",
    "    # El ejemplo usa encoding UTF-8. Esto se puede cambiar a otros encodings\n",
    "    # para textos con caracteres inusuales\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams) #, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    \n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade9b7",
   "metadata": {},
   "source": [
    "Es muy importante notar que el input de `texto_pdf()` **no es la ruta del archivo PDF sino la lectura (en bytes) del archivo**. Para obtener la lectura en bytes, tenemos que abrir los archivos primero. Para esto usaremos una funcion muy frecuente para abrir archivos en Python: `open()` combinado con la palabra clave `with`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abriremos el primer archivo en docs como ejemplo y lo usaremos con texto_PDF:\n",
    "ruta_documento = docs[0]\n",
    "with open(ruta_documento, 'rb') as f:\n",
    "    texto = texto_PDF(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c9ac6",
   "metadata": {},
   "source": [
    "Usaremos `print()` para visualizar el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32f0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8846f22",
   "metadata": {},
   "source": [
    "El resultado no es perfecto, pero funciona bastante bien para realizar analisis de textos. Ahora continuaremos con procesar todos los documentos en la lista `docs`. Este proceso podria tomar algo de tiempo en terminarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_completos = []\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    with open(doc, 'rb') as f:\n",
    "        \n",
    "        print(f'Leyendo documento {doc}...')\n",
    "        texto = texto_PDF(f)\n",
    "        textos_completos.append(texto)\n",
    "        print('\\tFinalizado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textos_completos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd8ab9",
   "metadata": {},
   "source": [
    "`textos_completos` ahora tiene los textos enteros de cada documento PDF listado en `docs`. Con esto, podemos seguir dando estructura a los textos en un dataframe de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e58a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be4883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anadiendo la ruta de los documentos como columna\n",
    "df_textos['archivos'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10743bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anadiendo los textos como columna\n",
    "df_textos['textos'] = textos_completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a64242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1383fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspeccionando una observacion\n",
    "df_textos['textos'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752a258",
   "metadata": {},
   "source": [
    "Ahora nuestros textos ya tienen una estructura de datos! con esto, podemos anadir nuevas columnas a `df_textos` con caracteristicas sobre los archivos que hemos leido. Para este ejemplo, anadiremos una columna con el numero de caracteres y otra con una variable dummy marcando cuales de los textos contienen las palabras clave \"IVA\" e \"impuestos\".\n",
    "\n",
    "### Numero de caracteres\n",
    "\n",
    "Para contar el numero de caracteres en cada texto usaremos el metodo `apply.()`. `.apply()` funciona de forma vectorizada, de modo que es mucho mas rapido que aplicar una funcion mediante un bucle. `.apply()` es similar al metodo `.map()` que se usa en Series de pandas, con la diferencia de que `.apply()` funciona en columnas de dataframes (`.map()` no se puede usar en dataframes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2163e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueva columna\n",
    "df_textos['n_caracteres'] = df_textos['textos'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a584071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6002580",
   "metadata": {},
   "source": [
    "### Palabras en el texto\n",
    "\n",
    "Nuestra siguiente columna sera una *dummy* (un valor que es uno o cero, donde el valor uno indica la presencia de una caracteristica) indicando que textos contienen las palabras clave \"IVA\" o \"impuestos\".\n",
    "\n",
    "Para esto, crearemos una funcion que toma un texto y verifica si las palabras clave estan en el. Luego usaremos `.apply()` para aplicar la funcion de forma vectorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_IVA_impuestos(texto):\n",
    "    \n",
    "    palabras = ['IVA', 'impuestos', 'Impuestos']\n",
    "    \n",
    "    for palabra in palabras:\n",
    "        \n",
    "        if palabra in texto:\n",
    "            \n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe13dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos['mencion_IVA_impuestos'] = df_textos['textos'].apply(palabras_IVA_impuestos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1677f",
   "metadata": {},
   "source": [
    "De esta forma podemos continuar agregando columnas al dataframe para proceder a analizar los textos. Esto es posible porque los datos ahora estan estructurados.\n",
    "\n",
    "Por ahora, dejaremos de trabajar con este dataframe para continuar los ejemplos de esta y la proxima sesion utilizando textos mas pequenos en lugar de documentos enteros. Todas las operaciones que veremos a continuacion se pueden aplicar en los textos de `df_textos` o en textos largos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-dr-training]",
   "language": "python",
   "name": "conda-env-python-dr-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
