{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66ebc20",
   "metadata": {},
   "source": [
    "### Nota importante:\n",
    "\n",
    "Este notebook utiliza el paquete `spacy`, que no es compatible con JupyterLite. Por tanto, ejecutar el codigo del notebook desde la URL del curso (https://dime-worldbank.github.io/taller-python-RD/lab/index.html) no es posible.\n",
    "\n",
    "Si deseas ejecutar este contenido, descarga el notebook y ejecutalo desde tu computadora con Jupyter Notebook o Jupyter Lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1677f",
   "metadata": {},
   "source": [
    "# 2. Limpieza y preparacion de datos de textos\n",
    "\n",
    "Para la siguiente parte, utilizaremos el archivo \"muestra_ejercicio_26_5.xlsx\". Este archivo contiene pequenos textos con el detalle de ventas reportadas por empresas en Republica Dominicana. Durante el resto de la sesion, vamos a trabajar con estos textos para modificarlos hasta que esten en una forma que sea util para analisis de datos de texto.\n",
    "\n",
    "Comenzaremos con explorar los datos para entender que contienen.\n",
    "\n",
    "### Exploracion de datos de texto\n",
    "\n",
    "La exploracion de datos de texto no es muy distinta a la exploracion de datos numericos. Basicamente consiste en visualizar un dataframe con datos de texto para familiarizarnos con lo que contiene.\n",
    "\n",
    "Empezaremos por cargar los datos. Por conveniencia, llamaremos a nuestro datadrame con los datos `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datos/muestra_ejercicio_26_5_2025.xlsx', sheet_name='Muestra_clasificada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f17db",
   "metadata": {},
   "source": [
    "Cada fila corresponde a un item reportado en el detalle de una boleta de venta. Para cada item, tenemos:\n",
    "- El nombre reportado\n",
    "- El tipo de ingreso por la venta\n",
    "- Si es un bien o servicio\n",
    "- La actividad del comercio emisor\n",
    "- El nombre generico del producto\n",
    "- El grupo del producto\n",
    "\n",
    "Vamos a tabular algunas de estas columnas para tener una idea de cuan frecuentes son las categorias que se muestran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indicador_bien_servicio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GENERICO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLASIFICACION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40907bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cb26e",
   "metadata": {},
   "source": [
    "La tabulacion de `nombre_item` indica varios puntos:\n",
    "- La columna tiene muchas categorias unicas, que no se repiten\n",
    "- La mayoria de caracteres estan en mayusculas\n",
    "- La mayoria de categorias describen en una o dos palabras el producto\n",
    "- Algunas tambien tienen informacion adicional en numeros o codigos\n",
    "\n",
    "Ahora que ya conocemos mejor los datos, podemos comenzar a planear que hacer con ellos. Para esto, vamos a suponer que tenemos dos objetivos para estos datos:\n",
    "\n",
    "- Hacer un analisis descriptivo de los textos en `nombre_item`.\n",
    "    - Contar palabras\n",
    "    - Hacer una nube de palabras\n",
    "- Contruir un clasificador automatico que toma los textos en `nombre_item` y determina si el item es de la categoria alimento o no.\n",
    "\n",
    "En concreto, realizaremos esto en la sesion de manana. Antes de eso, tenemos que preparar nuestros datos para que esten en una forma que resulta la mas adecuada para estos tipos de analisis.\n",
    "\n",
    "## 2.1 Normalizacion de datos de texto\n",
    "\n",
    "Para comprender por que la preparacion de textos es necesario, pensemos sobre lo siguiente?\n",
    "\n",
    "- Nuestros textos están en un estado muy \"crudo\". ¿No deberíamos \"limpiarlos\" un poco antes de contar palabras? Por ejemplo, palabras en minusculas y mayusculas se cuentan por separado  \n",
    "- Los textos en espanol (como en muchos otros idiomas) suelen repetir muchas palabras que no aportan mucho son muy informativas, como preposiciones, pronombres o contracciones. ¿Podemos eliminar algunas de ellas antes de contar palabras?  \n",
    "- Por último, ¿no deberíamos contar en la misma categoría palabras que no son exactamente iguales pero tienen un significado muy similar? Por ejemplo:\n",
    "    + diferentes conjugaciones del mismo verbo\n",
    "    + formas singulares y plurales del mismo sustantivo\n",
    "\n",
    "La respuesta a todas estas preguntas es **sí**. Lo haremos en la preparacion de los datos. La preparacion de datos en análisis de texto es **extremadamente importante**. Omitirlo puede darte resultados muy diferentes en tareas de análisis de texto.\n",
    "\n",
    "El preprocesamiento y la preparacion puede incluir múltiples tareas. Aplicaremos las siguientes a nuestros textos:\n",
    "\n",
    "- Convertir caracteres a minúsculas  \n",
    "- Tokenización: transformar textos en listas de palabras  \n",
    "- Eliminar palabras poco informativas. En lenguaje tecnico se conocen como *stop words*\n",
    "- Lematización: transformar diferentes formas de palabras en una forma común que exprese un significado similar. Esto es útil para \"normalizar\" conjugaciones de verbos o formas \n",
    "\n",
    "Por suerte, existe un paquete de Python muy útil que podemos usar para esto: [spaCy](https://spacy.io/).  \n",
    "spaCy pone a nuestra disposición modelos de NLP (procesamiento de lenguaje natural) que permiten tokenizar, lematizar y detectar *stop words* y caracteres que no son palabras (como dígitos o signos de puntuación), por lo que podemos transformar fácilmente un texto en una lista de palabras lematizadas \"significativas\" que podemos usar para contar palabras o construir clasificadores.\n",
    "\n",
    "### Trabajando con spaCy\n",
    "\n",
    "Primero necesitamos cargar spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala spaCy si no lo tienes:\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a81db",
   "metadata": {},
   "source": [
    "Ahora necesitamos **descargar** el modelo de NLP de spaCy en espanol. spaCy tiene varios modelos en espanol. Puedes verlos [aqui](https://spacy.io/models/es%20). Los modelos de mayor tamano funcionan mejor, pero tambien requieren mas espacio de disco duro y poder computacional. Nosotros trabajaremos con el modelo `es_core_news_md`, que no es muy pesado y logra buenos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d13e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c529e3",
   "metadata": {},
   "source": [
    "Ahora **cargamos** el modelo para que esté disponible en este notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96388eb0",
   "metadata": {},
   "source": [
    "Luego, construiremos una función que:\n",
    "\n",
    "1. Lee un texto  \n",
    "1. Lo transforme a minúsculas  \n",
    "1. Lo cargue al modelo de spaCy\n",
    "1. Para cada palabra de mas de 2 caracteres, obtiene la versión lematizada de aquellas que **no sean**:  \n",
    "    - *Stop words* (palabras sin mucho significado)\n",
    "    - Signos de puntuación  \n",
    "    - Números  \n",
    "    - Espacios  \n",
    "1. Finalmente, la función devuelve una lista de las palabras lematizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0143c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion_tokenizacion(texto):\n",
    "    \n",
    "    texto = texto.lower() # minusculas\n",
    "    doc = nlp(texto)      # cargando el texto al modelo de spaCy\n",
    "\n",
    "    palabras_normalizadas = []\n",
    "    for palabra in doc:\n",
    "        if palabra.text != '\\n' \\\n",
    "        and not palabra.is_stop \\\n",
    "        and not palabra.is_punct \\\n",
    "        and not palabra.like_num \\\n",
    "        and len(palabra.text.strip()) > 2:\n",
    "            lema = str(palabra.lemma_)\n",
    "            palabras_normalizadas.append(lema)\n",
    "    \n",
    "    return palabras_normalizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86997b",
   "metadata": {},
   "source": [
    "Para entender mejor que es la lema, veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'dominicanas'\n",
    "texto = nlp(texto)\n",
    "for palabra in texto:\n",
    "    print(palabra.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaea517",
   "metadata": {},
   "source": [
    "Para entender mejor lo que hace la función, veamos el resultado para un par de textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67501df",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df['nombre_item'][2]\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion_tokenizacion(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05744241",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df['nombre_item'][263]\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion_tokenizacion(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f75564",
   "metadata": {},
   "source": [
    "Notemos lo siguiente:\n",
    "\n",
    "- El resultado es una lista, no un texto\n",
    "- todas las palabras estan en minusculas\n",
    "- Las preposiciones han sido eliminadas\n",
    "- Los plurales han sido transformados a singular como parte de la lematizacion\n",
    "- Algunos femeninos han sido transformados a masculino tambien como parte de la lematizacion\n",
    "\n",
    "Para nuestra sorpresa, tambien podemos ver que la palabra `3x5` no ha sido eliminada. Esto lo corregiremos posteriormente mediante el uso de expresiones regulares.\n",
    "\n",
    "Por ahora, lo que nos queda es aplicar la funcion `normalizacion_tokenizacion()` de forma transversal a toda la columna `nombre_item`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560aae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_norm'] = df['nombre_item'].apply(normalizacion_tokenizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c47c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da44130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7465a",
   "metadata": {},
   "source": [
    "Visualizar estos resultados nos hace dar cuenta que hay mas codigos que no han sido eliminados en la normalizacion:\n",
    "\n",
    "- `3x5`\n",
    "- `rf-88`\n",
    "- `ib1707`\n",
    "\n",
    "Corregiremos esto mediante expresiones regulares.\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Patrones en textos y expresiones regulares\n",
    "\n",
    "Estos tres codigos parecen seguir patrones en los caracteres que contienen:\n",
    "\n",
    "- `3x5`: un numero, seguido de una `x`, seguida de un numero\n",
    "- `rf-88`: dos letras, seguidas de un guion, seguidas de dos numeros\n",
    "- `ib1707`: dos letras, seguidas de cuatros numeros\n",
    "\n",
    "Vamos a aprovechar estos patrones para eliminarlos del texto normalizado. Usaremos expresiones regulares para esto.\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "En programación, las expresiones regulares (*regex*) son secuencias de caracteres que coinciden con un patrón dentro de un texto. Un ejemplo simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re es el paquete en Python para trabajar con expresiones regulares.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'El número de DNI del contribuyente 1 es 30551. Nació el 1 de julio de 1976. El contribuyente 2 cuenta con DNI 71098'\n",
    "\n",
    "# Patrón para capturar los ID en este texto: secuencias de cinco caracteres numéricos.\n",
    "patron = '\\d{5}'\n",
    "\n",
    "# Capturando los DNI\n",
    "dni = re.findall(patron, texto)\n",
    "print(dni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0e4b",
   "metadata": {},
   "source": [
    "- `\\d` es un codigo representa un número (0-9). Esto es equivalente a `[0-9]`\n",
    "- `{5}` significa que el carácter anterior en el patrón se repite cinco veces\n",
    "- Una variación de este patrón podría ser `\\d{4}`, que podría usarse para capturar años. Esto habría devuelto una lista con `1996` en el ejemplo anterior\n",
    "\n",
    "En regex, existe un codigo para casi todo. Algunos ejemplos:\n",
    "\n",
    "- Codigos para caracteres:\n",
    "    + `\\d` --> dígitos (0-9)  \n",
    "    + `\\w` --> cualquier caracter de palabra (letras mayúsculas y minúsculas, dígitos numericos y el guion bajo `_`)  \n",
    "    + `\\n` --> saltos de línea  \n",
    "    + `\\s` --> caracteres de espacio en blanco, incluyendo saltos de línea  \n",
    "    + `.` --> cualquier caracter excepto el salto de línea (\\n)  \n",
    "\n",
    "- Para repetición de caracteres:\n",
    "    + `{a}` --> el carácter anterior, repetido exactamente \"a\" veces  \n",
    "    + `{a,b}` --> el carácter anterior, repetido entre \"a\" y \"b\" veces  \n",
    "    + `*` --> el carácter anterior, repetido cero o más veces  \n",
    "    + `+` --> el carácter anterior, repetido una o más veces  \n",
    "\n",
    "Regex puede detectar prácticamente cualquier patrón que podamos imaginar. Sin embargo, trabajar con expresiones regulares puede ser complejo al principio. En esta sesión hemos introducido el concepto de regex para que sepas que existe y que puede usarse para crear columnas en conjuntos de datos que contienen corpus de documentos.\n",
    "\n",
    "No te preocupes si todavía no entendiste bien cómo funcionan los patrones. Si te interesa aprender más sobre regex, te recomendamos los siguientes recursos:\n",
    "\n",
    "- Un buen tutorial sobre regex [aquí](https://regexone.com/)  \n",
    "- Una excelente herramienta visualizadora de expresiones regulares está [aquí](https://jex.im/regulex/#!flags=&re=www%5C.%5Ba-zA-Z0-9-%5D%2B%5C.(%3F%3Acom%7Cnet%7Corg))\n",
    "\n",
    "### Reemplazando información usando regex\n",
    "\n",
    "El comando `re.findall()` se usa para extraer todas las menciones de una regex. Nosotros usaremos `re.match()`, que se usa para reemplazar verificar si un patron existe en un texto.\n",
    "\n",
    "Nuestros patrones seran 3, de acuerdo a lo que ya hemos visto:\n",
    "\n",
    "- un numero, seguido de una `x`, seguida de un numero\n",
    "- dos letras minusculas, seguidas de un guion, seguidas de dos numeros\n",
    "- dos letras minusculas, seguidas de cuatros numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patron1 = '\\dx\\d'\n",
    "patron2 = '[a-z]{2}-\\d{2}'\n",
    "patron3 = '[a-z]{2}\\d{4}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15c504",
   "metadata": {},
   "source": [
    "Y haremos una funcion que borra estos patrones en `nombre_item_norm`, que era la columna con las palabras normalizadas y en listas. Recuerda que el input de esta funcion debe ser una lista, no un texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_codigos(lista_palabras):\n",
    "    \n",
    "    nueva_lista = []\n",
    "    \n",
    "    for palabra in lista_palabras:\n",
    "        \n",
    "        if not re.match(patron1, palabra) and \\\n",
    "            not re.match(patron2, palabra) and \\\n",
    "            not re.match(patron3, palabra):\n",
    "            \n",
    "            nueva_lista.append(palabra)\n",
    "    \n",
    "    return nueva_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15241bd5",
   "metadata": {},
   "source": [
    "Para comprobar su funcionamiento, probaremos la funcion en la observacion de indice 263 de `nombre_item_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_norm'][263]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar_codigos(df['nombre_item_norm'][263])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8074047",
   "metadata": {},
   "source": [
    "Ahora aplicaremos la funcion a toda la columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6428d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_final'] = df['nombre_item_norm'].apply(eliminar_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7903c90",
   "metadata": {},
   "source": [
    "Excelente! La columna resultante `nombre_item_final` contiene el texto preparado como lo vamos a necesitar para un analisis descriptivo y para construir un clasificador automatico basico.\n",
    "\n",
    "# 3. Guardando el resultado\n",
    "\n",
    "Este resultado contiene dos columnas que contienen valores en listas. Por tanto, no es posible guardar el dataframe como un CSV y preservar toda la informacion que esas listas contienen.\n",
    "\n",
    "Como alternativa, guardaremos el dataframe como un **pickle**.\n",
    "\n",
    "## Que es un pickle?\n",
    "\n",
    "Un pickle es un tipo de archivo que resulta de guardar una **variable** de Python (cualquier tipo de variable realmente) en bytes en nuestra computadora. Puede entenderse el tipo \"nativo\" de Python para guardar archivos, similar a como son los archivos `.xlsx` de Excel, `.dta` en Stata o `.Rds` para dataframes en R.\n",
    "\n",
    "## Como crear un pickle?\n",
    "\n",
    "Normalmente, el codigo para guardar un pickle es este:\n",
    "\n",
    "```{python}\n",
    "# Guardando una lista en un pickle\n",
    "mi_lista = [1, 2, 3, 4]\n",
    "\n",
    "import pickle\n",
    "with open('data.pkl', 'wb') as archivo:\n",
    "    pickle.dump(mi_lista, archivo)\n",
    "```\n",
    "\n",
    "Convenientemente, pandas con cuenta con un metodo que podemos aplicar directamente a un dataframe para guardar un pickle: `.to_pickle()`. Este funciona de forma muy similar a `.to_csv()`. Luego podemos usar la funcion de pandas `read_pickle()` para leer el pickle que guardamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be965caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_archivo = 'datos/detalle_ventas_clasificadas.pkl'\n",
    "df.to_pickle(ruta_archivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1666034",
   "metadata": {},
   "source": [
    "# Comentarios finales\n",
    "\n",
    "Antes de culminar, repasemos todos lo que hemos visto hoy:\n",
    "\n",
    "- Lectura de archivos PDF\n",
    "- Estructuracion de textos no estructurados en dataframes\n",
    "- Generacion de nuevas columnas sobre atributos del texto\n",
    "- Normalizacion de textos y tokenizacion\n",
    "- Expresiones regulares\n",
    "\n",
    "Todas estas tareas son tareas comunes en analisis de textos para preparar los datos. Manana veremos como utilizar estos datos preparados para realizar un analisis descriptivo y construir un clasificador. La sesion de manana empezara desde el pickle que hemos creado hoy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-dr-training]",
   "language": "python",
   "name": "conda-env-python-dr-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
