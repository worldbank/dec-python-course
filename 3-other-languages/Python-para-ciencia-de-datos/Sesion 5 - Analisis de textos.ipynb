{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/worldbank/dec-python-course/blob/main/3-other-languages/Python-para-ciencia-de-datos/Sesion%205%20-%20Analisis%20de%20textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7a0d0",
   "metadata": {},
   "source": [
    "# Sesion 5 - Analisis de texto descriptivo y clasificacion de textos\n",
    "\n",
    "Esta ultima sesion cubre los temas de analisis descriptivo de textos y clasificacion de textos.\n",
    "\n",
    "1. Analisis descriptivo de datos de textos\n",
    "    1. Conteo de palabras\n",
    "1. Clasificación de textos\n",
    "\n",
    "Esta sesión asume los conocimientos impartidos en las 4 sesiones previas: conocimiento basico de Python, conocimiento de pandas, y visualizacion de datos. La sesion parte desde el resultado generado en la sesion 4, que guardamos como un dataframe de pandas en formato pickle.\n",
    "\n",
    "Usaremos las siguientes bibliotecas en este notebook:\n",
    "\n",
    "- **seaborn**, **matplotlib** y **wordcloud** para visualización de datos  \n",
    "- **nltk** para análisis de sentimiento  \n",
    "- **sklearn** para clasificación de datos\n",
    "\n",
    "Empezaremos leyendo el dataframe que obtuvimos ayer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e350fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ruta = 'datos/detalle_ventas_clasificadas.pkl'\n",
    "df = pd.read_pickle(df_ruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6cec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f8267",
   "metadata": {},
   "source": [
    "# 1. Analisis descriptivo de textos\n",
    "\n",
    "## 1.1 Conteo de palabras\n",
    "\n",
    "Conteo de palabras consiste en contar:\n",
    "\n",
    "1. Cuantas palabras tiene un texto o corpus (conjunto de textos)\n",
    "1. Cuantas veces una palabra se repite en un texto o corpus\n",
    "\n",
    "Calcularemos ambos resultados en nuestro analisis. Para la primera tarea, crearemos directamente una columna con la longitud de la columna `nombre_item_final` en el dataframe.\n",
    "\n",
    "**Importante:** aunque es posible aplicar un conteo de palabras sobre textos sin preparacion (*raw*), este no es el mejor metodo para obtener una aproximacion a cuanta informacion contiene un texto ya que los textos sin preparacion incluyen *stop words*, codigos y palabras que no son realmente significativas en nuestro analisis. Por eso, siempre es mejor aplicar el conteo de palabras sobre un texto ya preparado.\n",
    "\n",
    "### Numero total de palabras en textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df410125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos el conteo con .apply(len)\n",
    "df['n_palabras'] = df['nombre_item_final'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ce9dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30076b94",
   "metadata": {},
   "source": [
    "Veamos ahora la distribucion del numero de palabras en los textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f89e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma con seaborn\n",
    "sns.histplot(data=df, x='n_palabras');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cf9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_palabras'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2d13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que observacion tendra 8 palabras?\n",
    "df[df['n_palabras']==8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedbbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observando el string en nombre_item\n",
    "df[df['n_palabras']==8]['nombre_item'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc7009",
   "metadata": {},
   "source": [
    "### Estimando cuantas veces las palabras se repiten a traves de todos los textos\n",
    "\n",
    "Para esta tarea, necesitamos generar una función que cree un diccionario donde cada clave es un palabra unica y cada valor un conteo de la palabra, para todos nuestros textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45512e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conteo_de_palabras_unicas(lista_de_palabras):\n",
    "    \n",
    "    conteo = {}\n",
    "    \n",
    "    for palabra in lista_de_palabras:\n",
    "        if palabra in conteo:  # esto verifica si la palabra ya existe en las claves de \"conteo\"\n",
    "            conteo[palabra] += 1\n",
    "        else:                  # si la palabra no existe, se anade 1 a su contador\n",
    "            conteo[palabra] = 1\n",
    "    \n",
    "    return conteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0038b4",
   "metadata": {},
   "source": [
    "Primero aplicaremos la función a un solo texto para asegurarnos de que el resultado se vea correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_palabras = df['nombre_item_final'][42]\n",
    "lista_de_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbeffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo = conteo_de_palabras_unicas(lista_de_palabras)\n",
    "conteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e0e55e",
   "metadata": {},
   "source": [
    "El resultado se ve correcto. Pero esto no es muy informativo:\n",
    "\n",
    "- nuestros textos en cada observacion de `nombre_item_final` son muy pequenos como para que un conteo de palabras a nivel de texto individual nos de informacion util\n",
    "- probablemente todos los conteos tengan una frecuencia de 1\n",
    "\n",
    "En lugar de aplicar esta funcion por texto individual, lo aplicaremos a todos los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenando todas las listas en nombre_item_final:\n",
    "nombre_item_total = df['nombre_item_final'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55599b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "nombre_item_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20e1ba5",
   "metadata": {},
   "source": [
    "Por que `.sum()` concatena todas las listas en `nombre_item_final`? `.sum()` es un metodo de pandas que suma todos los valores en una columna. En la sesion 1, sin embargo, vimos que Python permite ejecutar una operacion de **adicion de listas**: el resultado es listas concatenadas. Entonces, al ser aplicado a una columna con listas, `.sum()` resulta en una adicion de todas las listas en la columna, concatenandolas en una sola lista.\n",
    "\n",
    "Ahora aplicaremos la funcion a la lista con todas las palabras y guardaremos el resultado en `conteo_total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total = conteo_de_palabras_unicas(nombre_item_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd20c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005bdbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(conteo_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c683e2",
   "metadata": {},
   "source": [
    "Con esto, podemos graficar el conteo de palabras para todo nuestro corpus de artículos. Lo haremos a continuación para las *n* palabras más utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b547113",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "\n",
    "# Esta linea retorna los valores en conteo_total, en orden descendiente\n",
    "valores_descendentes = sorted(conteo_total.values())[::-1]\n",
    "\n",
    "valor_n = valores_descendentes[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69673a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "valor_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcda933",
   "metadata": {},
   "source": [
    "Esto significa que después de ordenar nuestros conteos de palabras en orden descendente, 8 es el valor en la posición 16 —recuerda que en Python las posiciones siempre empiezan en cero. Vamos a crear un bucle a traves del diccionario y nos quedaremos solo con los conteos mayores a este valor, guardando el resultado en un nuevo diccionario llamado `conteo_total_mayores_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total_mayores_n = {}\n",
    "for palabra, conteo in conteo_total.items():\n",
    "    if conteo > valor_n:\n",
    "        conteo_total_mayores_n[palabra] = conteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb88f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total_mayores_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conteo_total_mayores_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be782402",
   "metadata": {},
   "source": [
    "Ahora podemos producir nuestro plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b56e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total_mayores_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conteo_total_mayores_n['crema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = sorted(conteo_total_mayores_n, key=lambda x: conteo_total_mayores_n[x]) # anadimos esto para ordenar los resultados\n",
    "\n",
    "plot = sns.barplot(conteo_total_mayores_n, orient='h', order=sorting[::-1])\n",
    "plot.set(xticks=list(range(1, 20, 2)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764612e",
   "metadata": {},
   "source": [
    "## 1.2 Nube de palabras\n",
    "\n",
    "**Nota importante:** los dos paquetes que vamos a usar en el resto de la sesion, `wordcloud` y `scikit-learn`, no se pueden implementar en JupyterLite.\n",
    "\n",
    "¿Pero qué clase de taller sobre análisis de texto sería este sin un ejemplo de nube de palabras? Usaremos nuestro diccionario de conteos de palabras para el corpus de artículos y la librería `wordcloud` para esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activa e instala esta linea para instalar el paquete wordcloud\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12beb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud con las palabras en blanco y negro\n",
    "wc = WordCloud(background_color='white', colormap = 'binary').generate_from_frequencies(conteo_total)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e6e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con las palabras a color\n",
    "wc = WordCloud(background_color='white').generate_from_frequencies(conteo_total)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65ecbc",
   "metadata": {},
   "source": [
    "# 2. Clasificacion de textos\n",
    "\n",
    "Para la última parte de la sesión, haremos un par de ejemplos simples de clasificación de texto. Las llamamos \"simples\" porque hoy en día existen técnicas muy avanzadas para clasificación de texto, pero no son adecuadas para el tiempo que tenemos en esta sesión. Puedes consultar el enlace que aparece más abajo sobre LLMs si quieres explorar más sobre estos métodos (en ingles).\n",
    "\n",
    "En términos basicos, la clasificación de texto consiste en asignar un texto a un grupo. Si estás familiarizado con el aprendizaje automático (*machine learning*), clasificacion de textos es una tarea de clasificación. Para nuestro ejercicio, mostraremos dos formas de clasificar texto:\n",
    "1. **Clasificación supervisada:** agruparemos textos en grupos predefinidos. Los grupos predefinidos serán las clases `Alimentos`, `Bebidas alcohólicas` y `Prendas de vestir y calzado` de la columna `CLASIFICACION`. Todas las demas clases las agruparemos en la clase `Otros`. Hacemos esto porque las demas clases no cuentan con un numero de ejemplos suficientes para producir un clasificador aceptable.\n",
    "1. **Clasificación no supervisada:** agruparemos textos en grupos según su similitud, sin predefinir los grupos.\n",
    "\n",
    "**Notas sobre clasificacion supervisada:** \n",
    "- Un buen clasificador para clasificacion supervisada normalmente necesita algunos de miles de ejemplos **por clase** para lograr una clasificacion robusta. En este ejemplo, vamos a omitir ese detalle.\n",
    "- Clasificadores pre-entrenados con una tarea de analisis de texto generica, como los modelos BERT o RoBERTa, son una excepcion a esto y logran buenos resultados con algunos cientos de ejemplos por clase. Puedes leer mas sobre BERT y RoBERTa en el link al final de esta presentacion sobre LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995c844",
   "metadata": {},
   "source": [
    "La variable objectivo (*target*) es la clasificacion. Tendremos cuatro clases:\n",
    "- Alimentos\n",
    "- Bebidas alcoholicas\n",
    "- Prendas de vestir y calzado\n",
    "- Otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e3d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulacion de las clasificacion\n",
    "df['CLASIFICACION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ebe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de valores en clasificacion\n",
    "df['CLASIFICACION'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77478185",
   "metadata": {},
   "source": [
    "Para continuar, crearemos una nueva columna con las clases a predecir para la clasificacion supervisada. La llamaremos `clase`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a564b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clase alimentos\n",
    "df.loc[\n",
    "    df['CLASIFICACION'] == 'Alimentos',\n",
    "    'clase'\n",
    "] = 'alimentos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86fa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clase bebidas alcoholicas\n",
    "df.loc[\n",
    "    df['CLASIFICACION'] == 'Bebidas alcohólicas',\n",
    "    'clase'\n",
    "] = 'bebidas alcoholicas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287fb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clase prendas de vestir y clazado\n",
    "df.loc[\n",
    "    df['CLASIFICACION'] == 'Prendas de vestir y calzado',\n",
    "    'clase'\n",
    "] = 'prendas y calzado'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998df1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# las demas\n",
    "df.loc[\n",
    "    df['clase'].isna(),\n",
    "    'clase'\n",
    "] = 'otro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf30996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clase'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a7bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1a410",
   "metadata": {},
   "source": [
    "## 2.1 Codificación de textos (*text encoding*)\n",
    "\n",
    "Nuestro clasificador será construido (entrenado) usando los textos tokenizados y normalizados en `nombre_item_final`. Sin embargo, primero necesitamos convertirlos en números para que un clasificador pueda trabajar con ellos. Esta operación se llama **codificación** (*encoding*).\n",
    "\n",
    "Existen varias formas de codificar textos. Usaremos la frecuencia de término inversa a la frecuencia en documentos (TF-IDF: *Term Frequency-Inverse Document Frequency*). TF-IDF transforma un texto de palabras en un vector numérico donde cada palabra tiene una puntuación.\n",
    "- Palabras que aparecen con frecuencia en un texto reciben mayor puntuacion...\n",
    "- ... pero palabras que aparecen con mucha frecuencia en todos los documentos reciben una penalizacion.\n",
    "- Como resultado, **las palabras que son muy distintivas en uno o pocos textos en particular reciben mayor puntuacion**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66c8f1",
   "metadata": {},
   "source": [
    "Comenzaremos cargando la biblioteca que usaremos para la codificación y la clasificación de texto: `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para instalar scikit-learn:\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc34e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3639e7",
   "metadata": {},
   "source": [
    "Ahora creamos el codificador TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f485685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante: el input del codificador TF-IDF de scikit-learn es una lista con los textos\n",
    "corpus = list(df['nombre_item_final'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231813a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando el codificador\n",
    "codificador = TfidfVectorizer(stop_words = ['ref'], max_features=500)\n",
    "\n",
    "# Codificando\n",
    "vectores = codificador.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abb9c33",
   "metadata": {},
   "source": [
    "- el argumento `stop_words` nos permite anadir palabras que deben ser ignoradas por el codificador.\n",
    "    - Esto puede utilizarse si palabras sin significado fueron omitidas en la preparacion de datos\n",
    "- `max_features` indica cuanto es el maximo de palabras en el corpus que vamos a codificar. Recuerda que nuestro corpus tiene 2,000+ palabras, pero muchas de ellas solo se repiten una vez en los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fcc741",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conteo_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61306214",
   "metadata": {},
   "source": [
    "For an easier understanding of the text encoding, we'll transform this back into a dataframe:\n",
    "\n",
    "El objeto resultante `vectores` contiene las codificaciones de los 1,000 textos. Cada uno de ellos es un vector con la codificación TF-IDF de las 500 palabras más utilizadas en todo el corpus. Elegir 500 es una decisión arbitraria; recuerda que inicialmente teníamos un total de más de 2,000 palabras. De ahora en adelante, nos referiremos a estas 500 palabras como nuestro **diccionario**.\n",
    "\n",
    "Para entender mejor la codificación del corpus texto, transformaremos esto de nuevo en un dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e88c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas para el dataframe\n",
    "diccionario = codificador.get_feature_names_out()\n",
    "diccionario[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eedd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contenido del dataframe\n",
    "vectores_data = vectores.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data=vectores_data, columns=diccionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd0f314",
   "metadata": {},
   "source": [
    "Dos puntos importantes sobre este resultado:\n",
    "\n",
    "- La matriz `tf_idf` contiene la misma información que `vectores`, excepto que está transformada en un dataframe de Pandas con nombres de columnas e IDs de los artículos.\n",
    "    - Este paso no era realmente necesario pero lo añadimos para entender mejor el resultado de la codificación\n",
    "    - La mayoría de ejemplos de analisis de textos omitirán este paso y trabajarán directamente con `vectores`.\n",
    "- El resultado en `df_tfidf` y `vectores` es una matriz con **muchísimos** ceros.\n",
    "    - Esto sucede porque la codificación asigna un puntaje de cero a las palabras que están en el diccionario pero que no aparecen en ese documento. Este es un resultado esperable en la codificación TF-IDF.\n",
    "\n",
    "La información real en estos datos es escasa y se dispersa a lo largo de las muchas dimensiones (columnas) de los datos. Vamos a reducir la dimensionalidad de los datos con análisis de componentes principales (PCA) a solo dos dimensiones. Esto también nos permitirá visualizar la proximidad entre textos.\n",
    "\n",
    "## 2.2 Análisis de componentes principales (*PCA - Principal Component Analysis*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f962a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "PCA = sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2).fit(vectores) # inicializando y ajustando el modelo PCA\n",
    "vectores_reducidos = pca.transform(vectores)    # transformando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c26ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_reducidos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5677c3ac",
   "metadata": {},
   "source": [
    "El resultado ahora tiene solo dos valores por cada texto. Podemos visualizar los valores para entender mejor que paso aqui. En resumen, transformamos la matriz `df_tfidf` en esto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62891f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(vectores_reducidos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074e24f3",
   "metadata": {},
   "source": [
    "Podemos visualizar estos dos componentes resultantes. Los valores específicos no tienen ningun significado en concreto, pero la **proximidad entre los valores** indica que esos tenian tenían una codificación TF-IDF similar, lo que significa que están \"cerca\" en cuanto a las palabras que contienen.\n",
    "\n",
    "Además, vamos a usar en la visualizacion la variable `clase` para ver que tanto se asemejan textos pertenecientes a una misma clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura\n",
    "#fig = plt.figure(figsize = (10,6))\n",
    "plot = sns.scatterplot(\n",
    "    x = vectores_reducidos[:, 0], \n",
    "    y = vectores_reducidos[:, 1], \n",
    "    hue = df['clase'],\n",
    "    s = 10\n",
    ")\n",
    "\n",
    "# Personalizacion\n",
    "plt.legend(title='Clase')\n",
    "sns.move_legend(plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b24ec",
   "metadata": {},
   "source": [
    "## 2.3 Clasificacion supervisada\n",
    "\n",
    "Despues del resultado del PCA, podemos continuar con construir nuestros cladsificadores.\n",
    "\n",
    "Antes de seguir, pausemos un momento para repasar todos los pasos que hemos seguido para la clasificación de texto hasta este punto:\n",
    "\n",
    "1. Comenzamos con los textos en bruto e hicimos la preparación del texto:\n",
    "    - convertimos los textos a minúsculas\n",
    "    - eliminamos las palabras vacías (stop words) y los números\n",
    "    - lematizamos las palabras\n",
    "2. Luego codificamos los datos preparados usando TF-IDF\n",
    "3. Después, reducimos las dimensiones de 1000 a 2 y visualizamos el resultado\n",
    "\n",
    "Vamos a continuar usando los resultados del paso (3) para entrenar nuestros clasificadores supervisado y no supervisado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd3c3a",
   "metadata": {},
   "source": [
    "### Entrenamiento de un clasificador para clasificación supervisada\n",
    "\n",
    "Para asignar observaciones a grupos etiquetados, necesitamos hacer una **clasificación supervisada**. En este ejemplo usaremos un **clasificador de *random forest*** (bosque aleatorio), pero ten en cuenta que la biblioteca que estamos utilizando (**scikit-learn**) ofrece otros tipos de clasificadores tambien.\n",
    "\n",
    "- [Clasificadores de metodos de agrupacion de datos (*ensemble*)](https://scikit-learn.org/stable/api/sklearn.ensemble.html)\n",
    "- [Clasificadores de metodos bayesianos](https://scikit-learn.org/stable/api/sklearn.naive_bayes.html)\n",
    "- [Clasificadores de metodos de vectores de soporte (*support vector machine - SVM*)](https://scikit-learn.org/stable/api/sklearn.svm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bcd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs para el clasificador\n",
    "x = vectores_reducidos  # datos para la clasificacion\n",
    "y = df['clase']        # clases que vamos a predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c79b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando el clasificador\n",
    "clasificador = RandomForestClassifier(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108b45a",
   "metadata": {},
   "source": [
    "**Importante:** usar el argumento `class_weight` igual a `'balanced'` es crucial dado que esto ajusta la importancia de predecir clases sub representadas en `y`. Omitir esto da como resultado que el clasificador tienda a producir un mejor resultado para las clases mas representadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57993cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificador.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea10a2",
   "metadata": {},
   "source": [
    "Después de esto, el clasificador ha sido entrenado con los datos en `x` para aprender qué patrones en ellos producen los resultados en `y` (las clases).\n",
    "\n",
    "### Clasificación\n",
    "\n",
    "Ahora vamos a clasificar nuestros textos con el clasificador que entrenamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d704abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificaciones = classifier.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6491fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizando algunas de las clasificaciones\n",
    "clasificaciones[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e8f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulando los valores clasificados\n",
    "pd.Series(clasificaciones).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe947c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregando las clasificaciones al dataframe\n",
    "df['clasificacion'] = clasificaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisando las clases reales y clasificaciones\n",
    "pd.crosstab(df['clase'], df['clasificacion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8049066",
   "metadata": {},
   "source": [
    "Precision total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37460d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimando la precision total: (textos clasificados correctamente) / (total)\n",
    "precision = (df['clase'] == df['clasificacion']).sum() / len(df)\n",
    "print(f'La precision total es {precision*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a65f3c7",
   "metadata": {},
   "source": [
    "Precision por clases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d26655",
   "metadata": {},
   "outputs": [],
   "source": [
    "clases = list(df['clase'].unique())\n",
    "clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "for clase in clases:\n",
    "    df_temp = df[df['clase']==clase]\n",
    "    precision = (df_temp['clase'] == df_temp['clasificacion']).sum() / len(df_temp)\n",
    "    print(f'La precision de la clase {clase} es {round(precision*100, 1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4525a46e",
   "metadata": {},
   "source": [
    "Algunas notas sobre este resultado:\n",
    "\n",
    "- Nuestro clasificador tiene una precision total de 77%\n",
    "    - Este es un buen resultado, mas aun considerando que logramos esto con una muestra relativamente pequena y sin un proceso demasiado complicado\n",
    "- El clasificador predice mejor unas clases que otras. Esto suele pasar y depende de con que datos se ha entrenado el clasificador. En nuestro caso, los datos son valores numericos derivados de palabras mediante TF-IDF y PCA, asi que clases con palabras mas distintivas seguramente tendran una mejor clasificacion\n",
    "- Estamos evaluando el desempeno del clasificador con los mismos datos con los que lo entrenamos. Nota que hacemos esto solo por conveniencia, en realidad lo ideal seria dividir los datos totales de forma aleatoria entre una submuestra que usamos para entrenar el clasificador y otra submuestra que usamos para evaluar su desempeno --esto normalmente se conoce como los *train set* y *test set*. Por ejemplo, la division puede ser entre usar el 80% de los datos para el train set y el restanto 20% para el test set.\n",
    "    - Omitir esto puede llevar a producir un **clasificador sobreajustado** (*overfitted*): que funciona muy bien con los datos con los que se le entreno, pero no sirve para hacer generalizaciones en nuevos textos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d467e0",
   "metadata": {},
   "source": [
    "## 2.4 Clasificacion no supervisada\n",
    "\n",
    "La clasificacion no supervisada tiene la diferencia crucial de que los clasificadores no tienen una variable `y` con las clases que deben clasificar. Mas bien lo que hacen es tomar los datos en `x`, determinar que observaciones son \"cercanas\" en base a esos datos y asignar clases agnosticas de un significado predeterminado.\n",
    "\n",
    "El input del clasificador aca tambien seran los resultados del PCA en `vectores_reducidos`. Usaremos el método `KMeans()` del módulo `cluster` de la biblioteca `sklearn` para este clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa4d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c473fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de clases\n",
    "n = 4\n",
    "\n",
    "# Inicializando el clasificador\n",
    "km = sklearn.cluster.KMeans(n_clusters=n, init='k-means++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adf987",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.fit(vectores_reducidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0741180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(km.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebde84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['clase'], km.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294eb30a",
   "metadata": {},
   "source": [
    "Visualizando los resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21546290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figura\n",
    "#fig = plt.figure(figsize = (10,6))\n",
    "plot = sns.scatterplot(\n",
    "    x = vectores_reducidos[:, 0], \n",
    "    y = vectores_reducidos[:, 1], \n",
    "    hue = km.labels_,\n",
    "    s = 15\n",
    ")\n",
    "\n",
    "# Personalizacion\n",
    "plt.legend(title='Categoria')\n",
    "sns.move_legend(plot, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9bf63",
   "metadata": {},
   "source": [
    "Algunos comentarios:\n",
    "\n",
    "- La clasificacion no supervisada no produce grupos con un significado predefinido, como si ocurre en la clasificacion supervisada.\n",
    "    - Esto nos impide estimar la precision, ya que no sabemos exactamente que grupo puede ser analogo a cada categoria existente\n",
    "- El metodo que usamos, k-means, crea los grupos mas proximos de acuerdo a los datos en `x`. Esto se refleja en la visualizacion de arriba, donde hay un patron claro entre los grupos resultantes y los dos ejes (que son los numeros en `x`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8218cd",
   "metadata": {},
   "source": [
    "# Notas finales\n",
    "\n",
    "## Como mejorar un resultado como este\n",
    "\n",
    "- Mejora la preparacion de datos:\n",
    "    - Usa un language model de spaCy mas completo y que haga una lematizacion mas precisa\n",
    "    - Usa mas expresiones regulares para eliminar palabras que contienen codigos\n",
    "    - Introduce un espacio entre caracteres de letras y digitos en una misma palabray luego aplica la tokenizacion y lematizacion: esto eliminara codigos y letras sueltas que no tienen significado\n",
    "- Mejora la pre-clasificacion:\n",
    "    - Usa mas dimensiones en PCA. Prueba a ver como los resultados con 2-3 mas\n",
    "    - Anade mas columnas en `x`. En este ejemplo usamos solo 2, provenientes de PCA. Puedes anadir columnas con informacion que no utlilizamos del datafrmae `df`, como dummies por cada valor de algunas de las otras columnas\n",
    "- Mejora la clasificacion:\n",
    "    - Amplia la muestra a mas de 1,000 observaciones\n",
    "    - Separa la muestra en un *train set* y un *test set* para descartar que el modelo esta sobre sobreajustado\n",
    "    - Prueba otros tipos de clasificadores: esta vez utilizamos un modelo de bosques aleatorios, pero quizas otro modelo podria resultar mejor?\n",
    "\n",
    "## Siguientes pasos para este analisis\n",
    "\n",
    "- El archivo de Excel que usamos ayer, *muestra_ejercicio_26_5.xlsx*, tiene una pestana con datos sin clasificar.\n",
    "    - El siguiente paso seria aplicar todo el proceso de limpieza de datos de texto, codificacion TF-IDF y PCA para clasificar esos textos. Nota que la codificacion TF-IDF y el PCA que apliques debe ser **exactamente igual** que la que aplicamos justo antes de entrenar el modelo. Esto significa que no debes usar el metodo `.fit()` para preparar los datos, sino `.transform()`.\n",
    "\n",
    "## Otras tareas de análisis de texto\n",
    "\n",
    "Esta fue una introducción a tareas de análisis y mineria de textos. Otras tareas incluyen:\n",
    "\n",
    "- Reconocimiento de entidades nombradas (*Named Entity Recognition*): detectar menciones de entidades significativas (lugares, nombres de personas, fechas, etc.) en textos\n",
    "- Espacios vectoriales y *word embeddings*: transformar textos o palabras en vectores de \"significados\", en lugar de usar codificadores basados en la presencia de palabras como TF-IDF. Esto permite descomponer un texto en sus significados en lugar de en la presencia de palabras, lo cual es un mejor enfoque para muchas tareas de clasificacion **pero requiere mas poder computacional**\n",
    "- Analisis de sentimientos: consiste en detectar el tono emocional de un texto, usualmente una oracion o parrafo. No es muy relevante para este tipo de datos, pero es una tarea comun\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "\n",
    "No cubrimos los LLMs porque no forman parte de una sesión introductoria. Si te interesa aprender más sobre ellos, puedes revisar recomendamos estas lecturas (en ingles):\n",
    "\n",
    "- BERT fue el primer (¿o al menos uno de los primeros?) LLM publicado. Este artículo explica bien cómo funciona: [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n",
    "- Este es un tutorial sobre cómo trabajar con BERT y ajustarlo (*fine-tune*) para tareas específicas de análisis de texto: [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-dr-training]",
   "language": "python",
   "name": "conda-env-python-dr-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
