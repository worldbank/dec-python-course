{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeee32c8",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/worldbank/dec-python-course/blob/main/2-advanced-topics/text-analysis/intro-text-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23677227",
   "metadata": {},
   "source": [
    "# Introducción al Análisis de Texto\n",
    "\n",
    "Análisis de texto es el proceso de extraer información significativa a partir de datos textuales, revelando ideas que de otro modo permanecerían ocultas en grandes volumenes de texto. El termino \"texto\" aqui se refiere a cualquier conjunto de caracteres: podria desde libros enteros hasta una sola oracion o palabra.\n",
    "\n",
    "Esta sesión y la siguiente son una **introducción** al análisis de texto. Cubriremos los siguientes temas:\n",
    "\n",
    "1. Estructuracion de datos de texto no estructurados\n",
    "    1. Reconocimiento de caracteres desde documentos en PDF\n",
    "1. Limpieza y preparacion de datos de texto\n",
    "    1. Expresiones regulares y patrones de caracteres en datos de texto  \n",
    "    1. Preprocesamiento de datos textuales  \n",
    "1. Analisis descriptivo de datos de textos\n",
    "    1. Conteo de palabras\n",
    "    1. Análisis de sentimiento  \n",
    "1. Clasificación de textos\n",
    "\n",
    "Veremos los dos primeros puntos en la sesion de hoy y los dos ultimos puntos manana.\n",
    "\n",
    "Esta sesión asume conocimientos previos de Python y Pandas, así como cierto conocimiento de visualización de datos usando seaborn. Todo lo que cubrimos en las tres primeras sesiones de este taller es suficiente base para continuar con analisis de textos.\n",
    "\n",
    "Usaremos las siguientes bibliotecas en este notebook:\n",
    "\n",
    "- **pdfminer** para \"leer\" archivos PDF y extraer su contenido en textos\n",
    "- **pandas** para operaciones con dataframes  \n",
    "- **re** para expresiones regulares  \n",
    "- **spacy** para procesamiento de datos textuales\n",
    "\n",
    "# 1. Estructuracion de datos de texto no estructurados\n",
    "\n",
    "Los datos de en volumenes raramente tienen una estructura predeterminada. En muchos casos, estos vienen de archivos individuales que contienen textos. Por ejemplo, estos pueden ser un folder con decenas, cientos o miles de archivos en formato PDF, Word o `.txt`.\n",
    "\n",
    "Dar una estructura a estos archivos requiere que evaluemos cual es la mejor forma que una tabla de datos para cierto volumen de textos puede adquirir. Por ejemplo, al trabajar con un folder con cientos de archivos PDF de texto, podemos estructurar la tabla para que tenga una fila por documento, una fila fila por parrafo, una fila por oracion o incluso una fila por palabra.\n",
    "\n",
    "Para el siguiente ejemplo que usaremos en la primera mitad de esta sesion, partiremos de un volumen de documentos publicos del Banco Mundial. Estos documentos corresponden a algunos de los **reportes publicos que el Banco Mundial ha producido en Espanol sobre Republica Dominicana desde 2010 a 2024**.\n",
    "\n",
    "Estos documentos estan en la carpeta `docs/` y son todos archivos en PDF. La imagen debajo es una captura de pantalla del contenido de la carpeta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda85052",
   "metadata": {},
   "source": [
    "<img src=\"img/docs.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c77205",
   "metadata": {},
   "source": [
    "## 1.1 Reconocimiento optico de caracteres desde documentos en PDF\n",
    "\n",
    "El reconocimiento optico de caracteres (*Optical Character Recognition*--OCR por sus siglas en ingles) es una operacion comun en analisis de textos. Consiste en transformar textos que estan en algun documento (PDF or Word, por ejemplo) o en una imagen en un formato para texto que pueda ser operado por un lenguaje de programacion, por ejemplo en una string en Python.\n",
    "\n",
    "En este ejemplo, usaremos el paquete `pdfminer` para reconocer los caracteres de estos archivos PDF. Ten en cuenta que tambien es posible reconocer textos de archivos en Word o de imagenes a strings en Python.\n",
    "\n",
    "- [Mira aqui](https://github.com/microsoft/Simplify-Docx) un ejemplo (en ingles) para transformar archivos de Word a texto en Python\n",
    "- [Aqui](https://medium.com/do-it-with-code/extract-text-from-images-using-python-ocr-dc7092adf9a8) un ejemplo (tambien en ingles) para transformar imagenes con textos a strings en Python\n",
    "\n",
    "En general, existen muchas soluciones con metodos que logran resultados aceptables o buenos para reconocer caracteres de documentos o caracteres producidos con computadoras. Sin embargo, **reconocer escritura a mano**  es un proceso mucho mas complicado para el cual no existen soluciones gratuitas predeterminadas. El ejemplo que veremos en esta sesion y los dos links del parrafo anterior posiblemente no logren buenos resultados para caracteres escritos a mano.\n",
    "\n",
    "Ahora continuaremos importando los paquetes necesarios para \"leer\" un PDF a texto en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af623657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala PDF miner si no lo tienes\n",
    "# !pip install pdfminer\n",
    "\n",
    "# Modulos de pdfminer para leer PDF\n",
    "import pdfminer.pdfinterp\n",
    "import pdfminer.converter\n",
    "import pdfminer.layout\n",
    "import pdfminer.pdfpage\n",
    "\n",
    "# Paquetes para trabajar con directorios\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f985e",
   "metadata": {},
   "source": [
    "Empezaremos creando una lista con la ubicacion de todos los documentos que queremos leer desde PDF. Estos estan en la carpeta `doc/`. Para esto, vamos a usar el paquete `os` que nos permite interactuar con folderes para trabajar con archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca7291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder cuyos archivos queremos incluir en la lista\n",
    "folder = 'docs/'\n",
    "\n",
    "# Definiendo una lista vacia para agregar la ruta de los archivos PDF\n",
    "docs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538c9e6",
   "metadata": {},
   "source": [
    "El siguiente loop explora todos los archivos en `folder` y anade a la lista `docs` aquellos que tienen la extension `.pdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle a traves de los archivos en \"folder\"\n",
    "for archivo in os.listdir(folder):\n",
    "    if archivo.endswith(\".pdf\"):             # si el archivo es un PDF, continuamos\n",
    "        doc = os.path.join(folder, archivo)  # os.path.join une un nombre de carpeta y archivo para dar una ruta completa\n",
    "        print(f'Documento: {doc}')\n",
    "        docs.append(doc)                     # anadimos el archivo a la lista docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518028e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff55d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34f50d9",
   "metadata": {},
   "source": [
    "`docs` es ahora una lista con las rutas a los documentos PDF. Ahora vamos a iterar a traves de la lista para leerlos usando la funcion `texto_PDF()` que definiremos en el siguiente bloque.\n",
    "\n",
    "Esta funcion es bastante complicada, pero funciona bien para casi todos los casos en que cualquier usuario tendria que leer un PDF. No es necesario entender todo lo que contiene la funcion dado que algunos de estos comandos son bastante especializados. Para nuestro uso, no la modificaremos y la vamos a utilizar tal como esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52af512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_PDF(pdfFile):\n",
    "    \n",
    "    # Basado en codigo de http://stackoverflow.com/a/20905381/4955164\n",
    "    # El ejemplo usa encoding UTF-8. Esto se puede cambiar a otros encodings\n",
    "    # para textos con caracteres inusuales\n",
    "    codec = 'utf-8'\n",
    "    rsrcmgr = pdfminer.pdfinterp.PDFResourceManager()\n",
    "    retstr = io.StringIO()\n",
    "    layoutParams = pdfminer.layout.LAParams()\n",
    "    device = pdfminer.converter.TextConverter(rsrcmgr, retstr, laparams = layoutParams) #, codec = codec)\n",
    "    #We need a device and an interpreter\n",
    "    interpreter = pdfminer.pdfinterp.PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in pdfminer.pdfpage.PDFPage.get_pages(pdfFile, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    device.close()\n",
    "    returnedString = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    \n",
    "    return returnedString"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade9b7",
   "metadata": {},
   "source": [
    "Es muy importante notar que el input de `texto_pdf()` **no es la ruta del archivo PDF sino la lectura (en bytes) del archivo**. Para obtener la lectura en bytes, tenemos que abrir los archivos primero. Para esto usaremos una funcion muy frecuente para abrir archivos en Python: `open()` combinado con la palabra clave `with`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abriremos el primer archivo en docs como ejemplo y lo usaremos con texto_PDF:\n",
    "ruta_documento = docs[0]\n",
    "with open(ruta_documento, 'rb') as f:\n",
    "    texto = texto_PDF(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c9ac6",
   "metadata": {},
   "source": [
    "Usaremos `print()` para visualizar el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd32f0ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8846f22",
   "metadata": {},
   "source": [
    "El resultado no es perfecto, pero funcionaria bastante bien para realizar analisis de textos. Ahora continuaremos con procesar todos los documentos en la lista `docs`. Este proceso podria tomar algo de tiempo en terminarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_completos = []\n",
    "\n",
    "for doc in docs:\n",
    "    \n",
    "    with open(doc, 'rb') as f:\n",
    "        \n",
    "        print(f'Leyendo documento {doc}...')\n",
    "        texto = texto_PDF(f)\n",
    "        textos_completos.append(texto)\n",
    "        print('\\tFinalizado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textos_completos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd8ab9",
   "metadata": {},
   "source": [
    "`textos_completos` ahora tiene los textos enteros de cada documento PDF listado en `docs`. Con esto, podemos seguir dando estructura a los textos en un dataframe de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23d110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c842fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e58a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10743bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos['archivos'] = docs\n",
    "df_textos['textos'] = textos_completos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a64242",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752a258",
   "metadata": {},
   "source": [
    "Ahora nuestros textos ya tienen una estructura de datos! con esto, podemos anadir nuevas columnas a `df_textos` con caracteristicas sobre los archivos que hemos leido. Para este ejemplo, anadiremos una columna con el numero de caracteres y otra con una variable dummy marcando cuales de los textos contienen las palabras clave \"IVA\" e \"impuestos\".\n",
    "\n",
    "### Numero de caracteres\n",
    "\n",
    "Usaremos el metodo de pandas `.apply()`. Este metodo aplica una funcion a cada uno de los elementos de una columna de un dataframe.\n",
    "\n",
    "Recuerdas que mencionamos \"vectorizacion\" ayer? `.apply()` funciona de forma vectorizada, de modo que es mucho mas rapido que aplicar una funcion mediante un bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2163e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nueva columna\n",
    "df_textos['n_caracteres'] = df_textos['textos'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a584071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6002580",
   "metadata": {},
   "source": [
    "### Palabras en el texto\n",
    "\n",
    "Nuestra siguiente columna sera una *dummy* (un valor que es uno o cero, donde el valor uno indica la presencia de una caracteristica) indicando que textos contienen las palabras clave \"IVA\" o \"impuestos\".\n",
    "\n",
    "Para esto, crearemos una funcion que toma un texto y verifica si las palabras clave estan en el. Luego usaremos `.apply()` para aplicar la funcion de forma vectorizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_IVA_impuestos(texto):\n",
    "    \n",
    "    palabras = ['IVA', 'impuestos', 'Impuestos']\n",
    "    \n",
    "    for palabra in palabras:\n",
    "        \n",
    "        if palabra in texto:\n",
    "            \n",
    "            return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe13dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos['mencion_IVA_impuestos'] = df_textos['textos'].apply(palabras_IVA_impuestos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb1677f",
   "metadata": {},
   "source": [
    "De esta forma podemos continuar agregando columnas al dataframe para proceder a analizar los textos. Esto es posible porque los datos ahora estan estructurados.\n",
    "\n",
    "Por ahora, dejaremos de trabajar con este dataframe para continuar los ejemplos de esta y la proxima sesion utilizando textos mas pequenos en lugar de documentos enteros. Todas las operaciones que veremos a continuacion se pueden aplicar en los textos de `df_textos` o en textos largos.\n",
    "\n",
    "# 2. Limpieza y preparacion de datos de textos\n",
    "\n",
    "Para la siguiente parte, utilizaremos el archivo \"muestra_ejercicio.xlsx\". Este archivo contiene pequenos textos con el detalle de ventas reportadas por empresas en Republica Dominicana. Durante el resto de la sesion, vamos a trabajar con estos textos para modificarlos hasta que esten en una forma que sea util para analisis de datos de texto.\n",
    "\n",
    "Comenzaremos con explorar los datos para entender que contienen.\n",
    "\n",
    "### Exploracion de datos de texto\n",
    "\n",
    "La exploracion de datos de texto no es muy distinta a la exploracion de datos numericos. Basicamente consiste en visualizar un dataframe con datos de texto para familiarizarnos con lo que contiene.\n",
    "\n",
    "Empezaremos por cargar los datos. Por conveniencia, llamaremos a nuestro datadrame con los datos `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ca0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('datos/muestra_ejercicio.xlsx', sheet_name='Muestra_clasificada')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b0922",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2f17db",
   "metadata": {},
   "source": [
    "Cada fila corresponde a un item reportado en el detalle de una boleta de venta. Para cada item, tenemos:\n",
    "- El nombre reportado\n",
    "- El tipo de ingreso por la venta\n",
    "- Si es un bien o servicio\n",
    "- La actividad del comercio emisor\n",
    "- El nombre generico del producto\n",
    "- El grupo del producto\n",
    "\n",
    "Vamos a tabular algunas de estas columnas para tener una idea de cuan frecuentes son las categorias que se muestran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['indicador_bien_servicio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GENERICO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b35b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLASIFICACION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40907bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cb26e",
   "metadata": {},
   "source": [
    "La tabulacion de `nombre_item` indica varios puntos:\n",
    "- La columna tiene muchas categorias unicas, que no se repiten\n",
    "- La mayoria de caracteres estan en mayusculas\n",
    "- La mayoria de categorias describen en una o dos palabras el producto\n",
    "- Algunas tambien tienen informacion adicional en numeros o codigos\n",
    "\n",
    "Ahora que ya conocemos mejor los datos, podemos comenzar a planear que hacer con ellos. Para esto, vamos a suponer que tenemos dos objetivos para estos datos:\n",
    "\n",
    "- Hacer un analisis descriptivo de los textos en `nombre_item`.\n",
    "    - Contar palabras\n",
    "    - Hacer una nube de palabras\n",
    "- Contruir un clasificador automatico que toma los textos en `nombre_item` y determina si el item es de la categoria alimento o no.\n",
    "\n",
    "En concreto, realizaremos esto en la sesion de manana. Antes de eso, tenemos que preparar nuestros datos para que esten en una forma que resulta la mas adecuada para estos tipos de analisis.\n",
    "\n",
    "## 2.1 Normalizacion de datos de texto\n",
    "\n",
    "Para comprender por que la preparacion de textos es necesario, pensemos sobre lo siguiente?\n",
    "\n",
    "- Nuestros textos están en un estado muy \"crudo\". ¿No deberíamos \"limpiarlos\" un poco antes de contar palabras? Por ejemplo, palabras en minusculas y mayusculas se cuentan por separado  \n",
    "- Los textos en espanol (como en muchos otros idiomas) suelen repetir muchas palabras que no aportan mucho son muy informativas, como preposiciones, pronombres o contracciones. ¿Podemos eliminar algunas de ellas antes de contar palabras?  \n",
    "- Por último, ¿no deberíamos contar en la misma categoría palabras que no son exactamente iguales pero tienen un significado muy similar? Por ejemplo:\n",
    "    + diferentes conjugaciones del mismo verbo\n",
    "    + formas singulares y plurales del mismo sustantivo\n",
    "\n",
    "La respuesta a todas estas preguntas es **sí**. Lo haremos en la preparacion de los datos. La preparacion de datos en análisis de texto es **extremadamente importante**. Omitirlo puede darte resultados muy diferentes en tareas de análisis de texto.\n",
    "\n",
    "El preprocesamiento y la preparacion puede incluir múltiples tareas. Aplicaremos las siguientes a nuestros textos:\n",
    "\n",
    "- Convertir caracteres a minúsculas  \n",
    "- Tokenización: transformar textos en listas de palabras  \n",
    "- Eliminar palabras poco informativas. En lenguaje tecnico se conocen como *stop words*\n",
    "- Lematización: transformar diferentes formas de palabras en una forma común que exprese un significado similar. Esto es útil para \"normalizar\" conjugaciones de verbos o formas \n",
    "\n",
    "Por suerte, existe un paquete de Python muy útil que podemos usar para esto: [spaCy](https://spacy.io/).  \n",
    "spaCy pone a nuestra disposición modelos de NLP (procesamiento de lenguaje natural) que permiten tokenizar, lematizar y detectar *stop words* y caracteres que no son palabras (como dígitos o signos de puntuación), por lo que podemos transformar fácilmente un texto en una lista de palabras lematizadas \"significativas\" que podemos usar para contar palabras o construir clasificadores.\n",
    "\n",
    "### Trabajando con spaCy\n",
    "\n",
    "Primero necesitamos cargar spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d39d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala spaCy si no lo tienes:\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a81db",
   "metadata": {},
   "source": [
    "Now we need to **download** spaCy's NLP model. Uncomment the line below, run it only once, and then comment it out again to make sure you won't run it again accidentally.\n",
    "\n",
    "Ahora necesitamos **descargar** el modelo de NLP de spaCy en espanol. spaCy tiene varios modelos en espanol. Puedes verlos [aqui](https://spacy.io/models/es%20). Los modelos de mayor tamano funcionan mejor, pero tambien requieren mas espacio de disco duro y poder computacional. Nostros trabajaremos con el modelo `es_core_news_md`, que no es muy pesado y logra buenos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d13e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c529e3",
   "metadata": {},
   "source": [
    "Ahora **cargamos** el modelo para que esté disponible en este notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96388eb0",
   "metadata": {},
   "source": [
    "Then, we'll build a function that:\n",
    "\n",
    "1. Reads a text\n",
    "1. Transforms it to lowercase\n",
    "1. Loads it into the model\n",
    "1. For each word, obtains the lemmatized versions of words that are not:\n",
    "    - Stop words\n",
    "    - Punctuation\n",
    "    - Numbers\n",
    "    - Spaces\n",
    "1. Finally, the function returns a list of the lemmatized words\n",
    "\n",
    "Luego, construiremos una función que:\n",
    "\n",
    "1. Lew un texto  \n",
    "1. Lo transforme a minúsculas  \n",
    "1. Lo cargue al modelo de spaCy\n",
    "1. Para cada palabra de mas de 2 caracteres, obtiene la versión lematizada de aquellas que no sean:  \n",
    "    - *Stop words*  \n",
    "    - Signos de puntuación  \n",
    "    - Números  \n",
    "    - Espacios  \n",
    "1. Finalmente, la función devuelve una lista de las palabras lematizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0143c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizacion_tokenizacion(texto):\n",
    "    \n",
    "    texto = texto.lower() # minusculas\n",
    "    doc = nlp(texto)      # cargando el texto al modelo de spaCy\n",
    "\n",
    "    palabras_normalizadas = []\n",
    "    for palabra in doc:\n",
    "        if palabra.text != '\\n' \\\n",
    "        and not palabra.is_stop \\\n",
    "        and not palabra.is_punct \\\n",
    "        and not palabra.like_num \\\n",
    "        and len(palabra.text.strip()) > 2:\n",
    "            lema = str(palabra.lemma_)\n",
    "            palabras_normalizadas.append(lema)\n",
    "    \n",
    "    return palabras_normalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bbdf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'dominicanas'\n",
    "texto = nlp(texto)\n",
    "for palabra in texto:\n",
    "    print(palabra.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaea517",
   "metadata": {},
   "source": [
    "Para entender mejor lo que hace la función, veamos el resultado para un par de textos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67501df",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df['nombre_item'][1]\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118452fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion_tokenizacion(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05744241",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df['nombre_item'][74]\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizacion_tokenizacion(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f75564",
   "metadata": {},
   "source": [
    "Notemos lo siguiente:\n",
    "\n",
    "- El resultado es una lista, no un texto\n",
    "- todas las palabras estan en minusculas\n",
    "- Las preposiciones han sido eliminadas\n",
    "- Los plurales han sido transformados a singular como parte de la lematizacion\n",
    "\n",
    "Para nuestra sorpresa, tambien podemos ver que la palabra `3x5` no ha sido eliminada. Esto lo corregiremos posteriormente mediante el uso de expresiones regulares.\n",
    "\n",
    "Por ahora, lo que nos queda es aplicar la funcion `normalizacion_tokenizacion()` de forma transversal a toda la columna `nombre_item`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560aae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_norm'] = df['nombre_item'].apply(normalizacion_tokenizacionacion_tokenizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c47c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da44130",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7465a",
   "metadata": {},
   "source": [
    "Visualizar estos resultados nos hace dar cuenta que hay mas codigos que no han sido eliminados en la normalizacion:\n",
    "\n",
    "- `3x5`\n",
    "- `rf-88`\n",
    "- `ib1707`\n",
    "\n",
    "Corregiremos esto mediante expresiones regulares.\n",
    "\n",
    "\n",
    "\n",
    "## 2.2 Patrones en textos y expresiones regulares\n",
    "\n",
    "Estos tres codigos parecen seguir patrones en los caracteres que contienen:\n",
    "\n",
    "- `3x5`: un numero, seguido de una `x`, seguida de un numero\n",
    "- `rf-88`: dos letras, seguidas de un guion, seguidas de dos numeros\n",
    "- `ib1707`: dos letras, seguidas de cuatros numeros\n",
    "\n",
    "Vamos a aprovechar estos patrones para eliminarlos del texto normalizado. Usaremos expresiones regulares para esto.\n",
    "\n",
    "### Expresiones regulares\n",
    "\n",
    "En programación, las expresiones regulares (*regex*) son secuencias de caracteres que coinciden con un patrón dentro de un texto. Un ejemplo simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re es el paquete en Python para trabajar con expresiones regulares.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd5ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'El número de DNI del contribuyente 1 es 30551. Nació el 1 de julio de 1976. El contribuyente 2 cuenta con DNI 71098'\n",
    "\n",
    "# Patrón para capturar los ID en este texto: secuencias de cinco caracteres numéricos.\n",
    "patron = '\\d{5}'\n",
    "\n",
    "# Capturando los DNI\n",
    "dni = re.findall(patron, texto)\n",
    "print(dni)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b0e4b",
   "metadata": {},
   "source": [
    "- `\\d` es un codigo representa un número (0-9). Esto es equivalente a `[0-9]`\n",
    "- `{5}` significa que el carácter anterior en el patrón se repite cinco veces\n",
    "- Una variación de este patrón podría ser `\\d{4}`, que podría usarse para capturar años. Esto habría devuelto una lista con `1996` en el ejemplo anterior\n",
    "\n",
    "En regex, existe un codigo para casi todo. Algunos ejemplos:\n",
    "\n",
    "- Codigos para caracteres:\n",
    "    + `\\d` --> dígitos (0-9)  \n",
    "    + `\\w` --> cualquier caracter de palabra (letras mayúsculas y minúsculas, dígitos numericos y el guion bajo `_`)  \n",
    "    + `\\n` --> saltos de línea  \n",
    "    + `\\s` --> caracteres de espacio en blanco, incluyendo saltos de línea  \n",
    "    + `.` --> cualquier caracter excepto el salto de línea (\\n)  \n",
    "\n",
    "- Para repetición de caracteres:\n",
    "    + `{a}` --> el carácter anterior, repetido exactamente \"a\" veces  \n",
    "    + `{a,b}` --> el carácter anterior, repetido entre \"a\" y \"b\" veces  \n",
    "    + `*` --> el carácter anterior, repetido cero o más veces  \n",
    "    + `+` --> el carácter anterior, repetido una o más veces  \n",
    "\n",
    "Regex puede detectar prácticamente cualquier patrón que podamos imaginar. Sin embargo, trabajar con expresiones regulares puede ser complejo al principio. En esta sesión hemos introducido el concepto de regex para que sepas que existe y que puede usarse para crear columnas en conjuntos de datos que contienen corpus de documentos.\n",
    "\n",
    "No te preocupes si todavía no entendiste bien cómo funcionan los patrones. Si te interesa aprender más sobre regex, te recomendamos los siguientes recursos:\n",
    "\n",
    "- Un buen tutorial sobre regex [aquí](https://regexone.com/)  \n",
    "- Una excelente herramienta visualizadora de expresiones regulares está [aquí](https://jex.im/regulex/#!flags=&re=www%5C.%5Ba-zA-Z0-9-%5D%2B%5C.(%3F%3Acom%7Cnet%7Corg))\n",
    "\n",
    "### Reemplazando información usando regex\n",
    "\n",
    "El comando `re.findall()` se usa para extraer todas las menciones de una regex. Nosotros usaremos `re.match()`, que se usa para reemplazar verificar si un patron existe en un texto.\n",
    "\n",
    "Nuestros patrones seran 3, de acuerdo a lo que ya hemos visto:\n",
    "\n",
    "- un numero, seguido de una `x`, seguida de un numero\n",
    "- dos letras minusculas, seguidas de un guion, seguidas de dos numeros\n",
    "- dos letras minusculas, seguidas de cuatros numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "patron1 = '\\dx\\d'\n",
    "patron2 = '[a-z]{2}-\\d{2}'\n",
    "patron3 = '[a-z]{2}\\d{4}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15c504",
   "metadata": {},
   "source": [
    "Y haremos una funcion que borra estos patrones en `nombre_item_norm`, que era la columna con las palabras normalizadas y en listas. Recuerda que el input de esta funcion debe ser una lista, no un texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_codigos(lista_palabras):\n",
    "    \n",
    "    nueva_lista = []\n",
    "    \n",
    "    for palabra in lista_palabras:\n",
    "        \n",
    "        if not re.match(patron1, palabra) and \\\n",
    "            not re.match(patron2, palabra) and \\\n",
    "            not re.match(patron3, palabra):\n",
    "            \n",
    "            nueva_lista.append(palabra)\n",
    "    \n",
    "    return nueva_lista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15241bd5",
   "metadata": {},
   "source": [
    "Para comprobar su funcionamiento, probaremos la funcion en la segunda observacion de `nombre_item_norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204d622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_norm'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e1d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eliminar_codigos(df['nombre_item_norm'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8074047",
   "metadata": {},
   "source": [
    "Ahora aplicaremos la funcion a toda la columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6428d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nombre_item_final'] = df['nombre_item_norm'].apply(eliminar_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2395ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1666034",
   "metadata": {},
   "source": [
    "Excelente! La columna resultante `nombre_item_final` contiene el texto preparado como lo vamos a necesitar para un analisis descriptivo y para construir un clasificador automatico.\n",
    "\n",
    "# Comentarios finales\n",
    "\n",
    "Antes de culminar, repasemos todos lo que hemos visto hoy:\n",
    "\n",
    "- Lectura de archivos PDF\n",
    "- Estructuracion de textos no estructurados en dataframes\n",
    "- Generacion de nuevas columnas sobre atributos del texto\n",
    "- Normalizacion de textos y tokenizacion\n",
    "- Expresiones regulares\n",
    "\n",
    "Todas estas tareas son tareas comunes en analisis de textos para preparar los datos. Manana veremos como utilizar estos datos preparados para realizar un analisis descriptivo y construir un clasificador."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python-dr-training]",
   "language": "python",
   "name": "conda-env-python-dr-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
